linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
if (show_interpretation) {
cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
if (show_interpretation) {
cat("MKV 1: EDC en AGE zijn stochastic, dummy variables zijn deterministic\n")
}
plot(linear_model1$fitted.values, resid(linear_model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values",
ylab = "Residuals",
pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
# Updated upstream
# Residuen opslaan
residuals <- residuals(linear_model1)
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
labs(title = "Histogram van de Residuen", x = "Residuals", y = "Frequentie") +
theme_minimal()
# Q-Q plot
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)
if (show_interpretation) {
cat("Since the p value is small, we reject the null hypothesis .Therefore the residuals are NOT normally distributed.
Implications for OLS:
1) Unbiasedness: Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity,        exogeneity of regressors, and zero-mean error term) are satisfied
2) Efficiency:
OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator  (BLUE) under the Gauss-Markov assumptions.
3) Hypothesis Testing:
The validity of tests (e.g., (t)-tests and (F)-tests) and confidence intervals relies on the normality assumption. Non-normal residuals         can lead to incorrect p-values
4) Heteroscedasticity or Outliers:
Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.
Remediation:
Based on the histogram and Q-Q Plot which is slightly right skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.
"
)}
if (show_interpretation) {
cat("Gauss-Markov assumptions:
\n Assumption 1: Linearity in the parameters: CHECK
\n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
\n Correlation between explanatory variables and error terms:
")
df$residuals = residuals
cor(df[, c("residuals", "EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5", "REGION6",
"REGION7", "REGION8", "REGION9")])
}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
cat("Assumption 3: The expected V value of the error terms is zero: CHECK")
}
bptest(linear_model1)
if (show_interpretation) {
cat(" euuhhh , deze test zegt dat er geen heteroskedasticity is , maar onze residuals zijn wel niet nrml verdeeld lol . geen idee hoe ik verder moet ")
}
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
library(purrr)
# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
df <- df %>% mutate(
REGION1 = as.integer(REGION == 1),
REGION2 = as.integer(REGION == 2),
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),
QOB1 = as.integer(QOB == 1),
QOB2 = as.integer(QOB == 2),
QOB3 = as.integer(QOB == 3),
QOB4 = as.integer(QOB == 4)
)
if (isTRUE(show_interpretation)) {
cat("### **1. Empirical Specification and Hypotheses**\n\n")
cat("Baseline model:\n\n")
cat("$$WAGE_i = \\beta_1 + \\beta_2 \\, EDUCi + \\alpha \\, X_i + \\mu_i$$\n\n")
cat("Where Xi includes AGE, RACE, MARRIED, SMSA, and REGION.\n\n")
cat("### **Hypothesis Tests:**\n\n")
cat("EDUC: Education\n\n")
cat("H0: B2 $\\leq$ 0 → Education has no or a negative effect on wages.\n\n")
cat("H1: B2 $>$ 0 → Education has a positive effect on wages.\n\n")
cat("One-sided test: Economic theory predicts that more education increases wages, so a negative effect would be counterintuitive.\n\n")
cat("AGE: Age\n\n")
cat("H0: A(AGE) $\\leq$ 0 → Age has no or a negative effect on wages.\n\n")
cat("H1: A(AGE) $>$ 0 → Age has a positive effect on wages.\n\n")
cat("One-sided test: Age is often associated with experience, which is expected to increase wages.\n\n")
cat("MARRIED: Marital Status\n\n")
cat("H0: A(MARRIED) $\\leq$ 0 → Being married has no or a negative effect on wages.\n\n")
cat("H1: A(MARRIED) $>$ 0 → Being married has a positive effect on wages.\n\n")
cat("One-sided test: Being married may indicate stability and a positive impact on productivity, leading to higher wages.\n\n")
cat("SMSA: Metropolitan Area\n\n")
cat("H0: A(SMSA) $\\leq$ 0 → Living in a metropolitan area has no or a negative effect on wages.\n\n")
cat("H1: A(SMSA) $>$ 0 → Living in a metropolitan area has a positive effect on wages.\n\n")
cat("One-sided test: Urban areas typically have more job opportunities and higher wages, driving this expectation.\n\n")
cat("RACE: Race (1 = Black, 0 = White)\n\n")
cat("H0: A(RACE) $\\geq$ 0 → Race has no or a positive effect on wages.\n\n")
cat("H1: A(RACE) $<$ 0 → Race has a negative effect on wages.\n\n")
cat("One-sided test: Structural inequalities often result in lower wages for minority groups.\n\n")
cat("REGION: Geographic Region\n\n")
cat("H0: A(REGION) $\\neq$ 0 → (There are no positive wage differences across regions.)\n\n")
cat("H1: A(REGION) $=$ 0 → (At least one region has a positive effect on wages.)\n\n")
cat("Two-sided test: Region could impact wages in either direction\n\n")
}
cat("### **2. Explore the Data**\n\n")
head(df)
summary(df)
describe(df)
cov(df)
if (isTRUE(show_interpretation)) {
cat("### **Statistics per region**\n\n")  # Adds a Markdown header for clarity
cat("WAGE: Largest wage disparities in regions 2, 6, and 9; high SD, skewness, and kurtosis → outliers with very high incomes.\n\n")
cat("EDUC: Highest in region 9, lowest in region 6; slightly more variation in regions 5 and 6.\n\n")
cat("AGE: Almost uniformly ~45 years across regions; low spread, minimal differences.\n\n")
cat("RACE: Mostly white population (>90%); region 5 relatively more diversity; high kurtosis indicates outliers.\n\n")
cat("SMSA: Regions 4 & 6 are more urban, region 2 is the most rural; skewed → most live outside urban areas.\n\n")
cat("MARRIED: High marriage rates across all regions (~85–89%); region 9 slightly lower.\n\n")
}
library(dplyr)
library(psych)
df %>%
group_by(REGION) %>%
summarize(
mean_wage = mean(WAGE, na.rm = TRUE),
mean_educ = mean(EDUC, na.rm = TRUE),
mean_age = mean(AGE, na.rm = TRUE),
sd_wage = sd(WAGE, na.rm = TRUE),
sd_educ = sd(EDUC, na.rm = TRUE),
sd_age = sd(AGE, na.rm = TRUE),
var_wage = var(WAGE, na.rm = TRUE),
var_educ = var(EDUC, na.rm = TRUE),
var_age = var(AGE, na.rm = TRUE),
count = n()
)
#dit is hoe polina het toont in feedback (ook zelfde uitkomsten als haar)
#alleen de relevante kolommen (alle numerieke behalve dummies)
numeric_vars <- df %>%
select_if(is.numeric) %>%
select(-starts_with("REGION"), -starts_with("QOB"))  # dummies eruit
#combineer met regio
df_subset1 <- df %>%
select(REGION) %>%
bind_cols(numeric_vars)
#beschrijvende statistieken per regio
describeBy(df_subset1, group = "REGION")
if (isTRUE(show_interpretation)) {
cat("### **Statistics per age**\n\n")
cat("AGE GROUPS:\n\n")
cat("AGE >= 40 & AGE < 42 ~ '40-41 GROUP 1',\n\n")
cat("AGE >= 42 & AGE < 44 ~ '42-43 GROUP 2',\n\n")
cat("AGE >= 44 & AGE < 46 ~ '44-45 GROUP 3',\n\n")
cat("AGE >= 46 & AGE < 48 ~ '46-47 GROUP 4',\n\n")
cat("AGE >= 48 & AGE <= 50 ~ '48-50 GROUP 5'\n\n")
}
#dit is voor age groups
df_subset2 <- df %>%
mutate(AGE_GROUP = case_when(
AGE >= 40 & AGE < 42 ~ "40-41",
AGE >= 42 & AGE < 44 ~ "42-43",
AGE >= 44 & AGE < 46 ~ "44-45",
AGE >= 46 & AGE < 48 ~ "46-47",
AGE >= 48 & AGE <= 50 ~ "48-50"
)) %>%
select(-starts_with("REGION"), -starts_with("QOB"))
# Beschrijvende statistieken per AGE_GROUP
describeBy(df_subset2, group = "AGE_GROUP")
cat("### **Covariance matrices per regio**\n\n")
df %>%
group_by(REGION) %>%
group_split() %>%
map(~ {
region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
knitr::kable(cov(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED, QOB, QOB1:QOB4), use = "pairwise.complete.obs"),
caption = paste("Covariance Matrix for REGION:", region_name),
digits = 3)
})
cat("### **Correlation matrices per regio**\n\n")
df %>%
group_by(REGION) %>%
group_split() %>%
map(~ {
region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
knitr::kable(cor(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED, QOB, QOB1:QOB4),  use = "pairwise.complete.obs"),
caption = paste("Correlation Matrix for REGION:", region_name),
digits = 3)
})
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)")+
theme_minimal()
cat("### **3.Estimate the baseline Specification**\n\n")
#linear model
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3
+ REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  , data = df)
stargazer(linear_model1,type="text",style="all")
if (show_interpretation) {
cat("### Interpretation of the estimated Coefficient \n\n")
cat("\n EDUC (26.696, p < 0.001)
A one-year increase in education leads to a $26.696 increase in weelky wages.
This is highly significant (p = 0.000).
AGE ( 2.244, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).
RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).
SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.
MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).
Regional Effects on Wages
Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.
Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.
")
}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))
if (show_interpretation) {
cat("The joint significance test shows that AGE, RACE, MARRIED, and SMSA jointly contribute significantly to explaining WAGE.\n",
"The F-statistic is 60.23 with a p-value < 2.2e-16, indicating strong statistical significance.\n",
"We reject the null hypothesis that these four variables have no joint effect on wages.\n")
}
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
if (show_interpretation) {
cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
cat("### **4. Evaluating Gauss-Markov Assumptions and Applying Remedial Measures**\n\n")
cat("#### **4a. Stochastic Regressors**\n\n")
if (show_interpretation) {
cat("All the variables are stochastic.\n\n")
cat("
Strictly exogenous: AGE, RACE, QOB
- These variables are fixed and not influenced by wages or the error term.
Weakly exogenous: SMSA, REGION
- These variables may be correlated with unobserved factors affecting wages, but are not directly influenced by wages.
Endogenous: EDUC, MARRIED
- Potential for reverse causality or correlation with the error term (e.g., higher wages → more education or higher likelihood of marriage).
")
}
plot(linear_model1$fitted.values, resid(linear_model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values",
ylab = "Residuals",
pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
# Updated upstream
# Residuen opslaan
residuals <- residuals(linear_model1)
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
labs(title = "Histogram Residuals", x = "Residuals", y = "Frequentie") +
theme_minimal()
cat("#### **4b. Normality Error Terms**\n\n")
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)
if (show_interpretation) {
cat(paste(
"Since the p value is small, we reject the null hypothesis. Therefore the residuals are NOT normally distributed.",
"",
"Implications for OLS:",
"",
"1) Unbiasedness:",
"Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity, exogeneity of regressors, and zero-mean error term) are satisfied.",
"",
"2) Efficiency:",
"OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov assumptions.",
"",
"3) Hypothesis Testing:",
"The validity of tests (e.g., t-tests and F-tests) and confidence intervals relies on the normality assumption. Non-normal residuals can lead to incorrect p-values.",
"",
"4) Heteroscedasticity or Outliers:",
"Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.",
"",
"Remediation:",
"Based on the histogram and Q-Q Plot which is slightly right-skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.",
sep = "\n"
))
}
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)
if (show_interpretation) {
cat("Since the p value is small, we reject the null hypothesis .Therefore the residuals are NOT normally distributed.
Implications for OLS:
1) Unbiasedness: Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity,        exogeneity of regressors, and zero-mean error term) are satisfied
2) Efficiency:
OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator  (BLUE) under the Gauss-Markov assumptions.
3) Hypothesis Testing:
The validity of tests (e.g., (t)-tests and (F)-tests) and confidence intervals relies on the normality assumption. Non-normal residuals         can lead to incorrect p-values
4) Heteroscedasticity or Outliers:
Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.
Remediation:Based on the histogram and Q-Q Plot which is slightly right skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies."
)}
if (show_interpretation) {
cat("Gauss-Markov assumptions:
\n Assumption 1: Linearity in the parameters: CHECK
\n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
\n Correlation between explanatory variables and error terms:
")
}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
cat("Assumption 3: The expected V value of the error terms is zero: CHECK")
}
bptest(linear_model1)
if (show_interpretation) {
cat(" geen heteorskedasticity")
}
if (show_interpretation) {
cat("")
}
if (show_interpretation) {
cat("Multicollinearity")
}
if (show_interpretation) {
cat("Multicollinearity")
}
if (show_interpretation) {
cat("Multicollinearity")
# Assuming your dataset is named 'df'
correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5",
"REGION6", "REGION7", "REGION8", "REGION9")])
print(correlation_matrix)
}
install.packages("corrplot")
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
library(purrr)
library(corrplot)
# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
df <- df %>% mutate(
REGION1 = as.integer(REGION == 1),
REGION2 = as.integer(REGION == 2),
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),
QOB1 = as.integer(QOB == 1),
QOB2 = as.integer(QOB == 2),
QOB3 = as.integer(QOB == 3),
QOB4 = as.integer(QOB == 4)
)
if (show_interpretation) {
cat("Multicollinearity")
# Assuming your dataset is named 'df'
correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5",
"REGION6", "REGION7", "REGION8", "REGION9")])
print(correlation_matrix)
corrplot(correlation_matrix, method = "circle")
}
vif_values -> vif(linear_model1)
vif_values = vif(linear_model1)
print(vif_values)
if (show_interpretation) {
cat("Multicollinearity")
correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5",
"REGION6", "REGION7", "REGION8", "REGION9")])
print(correlation_matrix)
corrplot(correlation_matrix, method = "cube")
}
if (show_interpretation) {
cat("Multicollinearity")
correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5",
"REGION6", "REGION7", "REGION8", "REGION9")])
print(correlation_matrix)
corrplot(correlation_matrix, method = "circle")
}
print(correlation_matrix)
if (show_interpretation) {
cat("Multicollinearity\n
Gauss-Markov assumption (no perfect multicollinearity) is met.
\n\n
Analysis of Variance Inflation Factors:
Low VIFs (<5): Most of the variables have VIF values close to 1, including EDUC (1.07), AGE (1.01), RACE (1.05), SMSA (1.07), and MARRIED (1.02). These values suggest minimal multicollinearity, meaning these predictors are relatively independent.\n
Moderate VIFs (Between 2 and 5): Some regional variables—REGION2 (3.28), REGION3 (3.67), REGION4 (2.18), REGION5 (3.51), REGION6 (2.06), REGION7 (2.54), REGION8 (1.82), and REGION9 (2.92)—show moderate correlation with other predictors. These values indicate some degree of collinearity, but they are not alarmingly high.
\n
High VIFs (>5): None of the variables exceed 5, which suggests that severe multicollinearity is not a major issue in this regression model. But even if VIF would be high (VIF>5), a large sample (n = 10 000 in this case) and high variance
in the explanatory variables can still lead to precise estimates.
Implications for the properties and precision of the OLS estimator:
1) Parameters remain identifiable.\n
2) Under the CNLRM assumptions, the OLS estimator remains BLUE and normally distributed.\n
3) OLS estimators exhibit larger variance and covariance, especially the regions.\n
4) Wider confidence intervals and lower t-statistics
->variables appear less significantly different from zero, higher probability of making a Type II error\n
5) The overall model fit (R2) is largely unaffected: even if individual coefficients are insignificant, the F -test may
indicate that the coefficients are jointly significant, and can be estimated with high precision.\n
6) Regression coefficients may change substantially\n
")
correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5",
"REGION6", "REGION7", "REGION8", "REGION9")])
corrplot(correlation_matrix, method = "circle")
}
```{r}
if (show_interpretation) {
cat("Multicollinearity\n
Gauss-Markov assumption (no perfect multicollinearity) is met.
\n\n
Analysis of Variance Inflation Factors:
Low VIFs (<5): Most of the variables have VIF values close to 1, including EDUC (1.07), AGE (1.01), RACE (1.05), SMSA (1.07), and MARRIED (1.02). These values suggest minimal multicollinearity, meaning these predictors are relatively independent.\n
Moderate VIFs (Between 2 and 5): Some regional variables—REGION2 (3.28), REGION3 (3.67), REGION4 (2.18), REGION5 (3.51), REGION6 (2.06), REGION7 (2.54), REGION8 (1.82), and REGION9 (2.92)—show moderate correlation with other predictors. These values indicate some degree of collinearity, but they are not alarmingly high.
\n
High VIFs (>5): None of the variables exceed 5, which suggests that severe multicollinearity is not a major issue in this regression model. But even if VIF would be high (VIF>5), a large sample (n = 10 000 in this case) and high variance
in the explanatory variables can still lead to precise estimates.
Implications for the properties and precision of the OLS estimator:
1) Parameters remain identifiable.\n
2) Under the CNLRM assumptions, the OLS estimator remains BLUE and normally distributed.\n
3) OLS estimators exhibit larger variance and covariance, especially the regions.\n
4) Wider confidence intervals and lower t-statistics
->variables appear less significantly different from zero, higher probability of making a Type II error\n
5) The overall model fit (R2) is largely unaffected: even if individual coefficients are insignificant, the F -test may
indicate that the coefficients are jointly significant, and can be estimated with high precision.\n
6) Regression coefficients may change substantially\n
")
correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5",
"REGION6", "REGION7", "REGION8", "REGION9")])
corrplot(correlation_matrix, method = "circle")
}
```{r}
if (show_interpretation) {
cat("Multicollinearity\n
Gauss-Markov assumption (no perfect multicollinearity) is met.
\n\n
Analysis of Variance Inflation Factors:
Low VIFs (<5): Most of the variables have VIF values close to 1, including EDUC (1.07), AGE (1.01), RACE (1.05), SMSA (1.07), and MARRIED (1.02). These values suggest minimal multicollinearity, meaning these predictors are relatively independent.\n
Moderate VIFs (Between 2 and 5): Some regional variables—REGION2 (3.28), REGION3 (3.67), REGION4 (2.18), REGION5 (3.51), REGION6 (2.06), REGION7 (2.54), REGION8 (1.82), and REGION9 (2.92)—show moderate correlation with other predictors. These values indicate some degree of collinearity, but they are not alarmingly high.
\n
High VIFs (>5): None of the variables exceed 5, which suggests that severe multicollinearity is not a major issue in this regression model. But even if VIF would be high (VIF>5), a large sample (n = 10 000 in this case) and high variance
in the explanatory variables can still lead to precise estimates.
Implications for the properties and precision of the OLS estimator:
1) Parameters remain identifiable.\n
2) Under the CNLRM assumptions, the OLS estimator remains BLUE and normally distributed.\n
3) OLS estimators exhibit larger variance and covariance, especially the regions.\n
4) Wider confidence intervals and lower t-statistics
->variables appear less significantly different from zero, higher probability of making a Type II error\n
5) The overall model fit (R2) is largely unaffected: even if individual coefficients are insignificant, the F -test may
indicate that the coefficients are jointly significant, and can be estimated with high precision.\n
6) Regression coefficients may change substantially\n
")
correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5",
"REGION6", "REGION7", "REGION8", "REGION9")])
corrplot(correlation_matrix, method = "circle")
}
print(correlation_matrix)
vif_values = vif(linear_model1)
print(vif_values)
#VIF values
vif_values = vif(linear_model1)
print(vif_values)
print('IF values')
vif_values = vif(linear_model1)
print(vif_values)
print('vIF values')
vif_values = vif(linear_model1)
print(vif_values)
print('VIF values')
vif_values = vif(linear_model1)
print(vif_values)
print(correlation_matrix)
print(max(abs(correlation_matrix)))
print(correlation_matrix)
print(max(abs(correlation_matrix)))
which(abs(cor_matrix) == max_corr, arr.ind = TRUE)
