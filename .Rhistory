if (show_interpretation) {
cat("Assumption 3: The expected V value of the error terms μi is zero: CHECK")
}
if (show_interpretation) {
cat("Gauss-Markov assumptions:
\n Assumption 1: Linearity in the parameters: CHECK
\n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model)
")
cor_test_results <- sapply(df[, c("EDUC", "AGE")], function(x) cor.test(x, residuals)$p.value)
print(cor_test_results)
summary(df[df$YEAR == 2000, c("EDUC", "AGE")])
summary(df[df$YEAR == 2010, c("EDUC", "AGE")])
}
if (show_interpretation) {
cat("Gauss-Markov assumptions:
\n Assumption 1: Linearity in the parameters: CHECK
\n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model)
")
cor_test_results <- sapply(df[, c("EDUC", "AGE")], function(x) cor.test(x, residuals)$p.value)
print(cor_test_results)
}
install.packages("tinytex")
tinytex::is_tinytex()
install.packages("tinytex")
update.packages(ask = FALSE, checkBuilt = TRUE)
update.packages(ask = FALSE, checkBuilt = TRUE)
tinytex::tlmgr_update()
install.packages(tlmgr)
install.packages("tlmgr")
tinytex::install_tinytex()
tinytex::is_tinytex()
tinytex::tlmgr_path()
tinytex::tlmgr_path()
tinytex::tlmgr_path()
Sys.setenv(PATH = paste0("C:/Users/Bálint/AppData/Roaming/TinyTeX/bin/win32", ";", Sys.getenv("PATH")))
Sys.setenv(PATH = paste0("C:/Users/Bálint/AppData/Roaming/TinyTeX/bin/win32", ";", Sys.getenv("PATH"))) tinytex::tlmgr_path()
> tinytex::tlmgr_path()
tinytex::tlmgr_path()
Sys.setenv(PATH = paste0("C:/Users/Bálint/AppData/Roaming/TinyTeX/bin/win32", ";", Sys.getenv("PATH"))) tinytex::tlmgr_path()
Sys.setenv(PATH = paste0("C:/Users/Balint/AppData/Roaming/TinyTeX/bin/win32", ";", Sys.getenv("PATH"))) tinytex::tlmgr_path()
Sys.setenv(PATH = paste0("C:/Users/Bálint/AppData/Roaming/TinyTeX/bin/win32", ";", Sys.getenv("PATH"))) tinytex::tlmgr_path()
tinytex::tlmgr_path()
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
summary(df)
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
if (isTRUE(show_interpretation)) {
cat("### **1.Hypothesis Testing**\n\n")  # Adds a Markdown header for clarity
cat("**H0:** B2 = 0 → Education has no effect on wages.\n\n")
cat("**H1:** B2 > 0 → Education has a positive effect on wages.\n\n")
cat("One-sided hypotheses are appropriate in this case because education is expected to increase earnings (according to economic theories).
In addition, there’s often little reason to test whether more education reduces wages because that result is counterintuitive.")
}
cat("### **2. Explore the Data**\n\n")
head(df)
summary(df)
describe(df)
cov(df)
cor(df)
#Creation of dummy vars for the regions and QOBs
df <- df %>%
mutate(
REGION1 = as.integer(REGION == 1),
REGION2 = as.integer(REGION == 2),
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),
QOB1 = as.integer(QOB == 1),
QOB2 = as.integer(QOB == 2),
QOB3 = as.integer(QOB == 3),
QOB4 = as.integer(QOB == 4)
)
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 1200) +
theme_minimal()
cat("### **3.Estimate the baseline Specification**\n\n")
#linear model
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  + QOB2 + QOB3 + QOB4, data = df)
stargazer(linear_model1,type="text",style="all")
if (show_interpretation) {
cat("### Interpretation of the estimated Coefficient \n\n")
cat("\n EDUC (26.729, p < 0.001)
A one-year increase in education leads to a $26.73 increase in wages.
This is highly significant (p = 0.000).
AGE (2.226, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).
RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).
SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.
MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).
Regional Effects on Wages
Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.
Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.
QOB:
being born in Q2 is associated with a $3.94 lower wage compared to Q1,
Being born in Q3 is associated with a $4.79 higher wage compared to Q1, while Being born in Q4 is associated with a $3.73 lower wage compared to Q1.None of the QOB coefficients are significant, meaning quarter of birth does not have a meaningful impact on wages in this model.
The p-values (all > 0.5) suggest that there is no strong evidence that being born in a particular quarter affects wages
While the model is statistically significant, it explains only 13.4% of the variation in wages. Other unobserved factors (e.g. experience, skills) likely play a major role."
)
}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
if (show_interpretation) {
cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
if (show_interpretation) {
cat("MKV 1: EDC en AGE zijn stochastic, dummy variables zijn deterministic\n")
}
plot(linear_model1$fitted.values, resid(linear_model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values",
ylab = "Residuals",
pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
# Updated upstream
# Residuen opslaan
residuals <- residuals(linear_model1)
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
labs(title = "Histogram van de Residuen", x = "Residuals", y = "Frequentie") +
theme_minimal()
# Q-Q plot
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)
if (show_interpretation) {
cat("Since the p value is small, we reject the null hypothesis .Therefore the residuals are NOT normally distributed.
Implications for OLS:
1) Unbiasedness: Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity,        exogeneity of regressors, and zero-mean error term) are satisfied
2) Efficiency:
OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator  (BLUE) under the Gauss-Markov assumptions.
3) Hypothesis Testing:
The validity of tests (e.g., (t)-tests and (F)-tests) and confidence intervals relies on the normality assumption. Non-normal residuals         can lead to incorrect p-values
4) Heteroscedasticity or Outliers:
Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.
Remediation:
Based on the histogram and Q-Q Plot which is slightly right skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.
"
)}
if (show_interpretation) {
cat("Gauss-Markov assumptions:
\n Assumption 1: Linearity in the parameters: CHECK
\n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
\n Correlation between explanatory variables and error terms:
")
df$residuals = residuals
cor(df[, c("residuals", "EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5", "REGION6",
"REGION7", "REGION8", "REGION9", "QOB2", "QOB3", "QOB4")])
}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
cat("Assumption 3: The expected V value of the error terms is zero: CHECK")
}
bptest(linear_model1)
if (show_interpretation) {
cat(" euuhhh , deze test zegt dat er geen heteroskedasticity is , maar onze residuals zijn wel niet nrml verdeeld lol . geen idee hoe ik verder moet ")
}
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 50000) +
theme_minimal()
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 15000) +
theme_minimal()
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 13000) +
theme_minimal()
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 15000) +
theme_minimal()
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 12000) +
theme_minimal()
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
if (isTRUE(show_interpretation)) {
cat("### **1.Hypothesis Testing**\n\n")  # Adds a Markdown header for clarity
cat("**H0:** B2 = 0 → Education has no effect on wages.\n\n")
cat("**H1:** B2 > 0 → Education has a positive effect on wages.\n\n")
cat("One-sided hypotheses are appropriate in this case because education is expected to increase earnings (according to economic theories).
In addition, there’s often little reason to test whether more education reduces wages because that result is counterintuitive.")
}
cat("### **2. Explore the Data**\n\n")
head(df)
summary(df)
describe(df)
cov(df)
cor(df)
#Creation of dummy vars for the regions and QOBs
df <- df %>%
mutate(
REGION1 = as.integer(REGION == 1),
REGION2 = as.integer(REGION == 2),
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),
)
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 12000) +
theme_minimal()
cat("### **3.Estimate the baseline Specification**\n\n")
#linear model
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9, data = df)
stargazer(linear_model1,type="text",style="all")
if (show_interpretation) {
cat("### Interpretation of the estimated Coefficient \n\n")
cat("\n EDUC (26.729, p < 0.001)
A one-year increase in education leads to a $26.73 increase in wages.
This is highly significant (p = 0.000).
AGE (2.226, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).
RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).
SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.
MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).
Regional Effects on Wages
Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.
Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.
QOB:
being born in Q2 is associated with a $3.94 lower wage compared to Q1,
Being born in Q3 is associated with a $4.79 higher wage compared to Q1, while Being born in Q4 is associated with a $3.73 lower wage compared to Q1.None of the QOB coefficients are significant, meaning quarter of birth does not have a meaningful impact on wages in this model.
The p-values (all > 0.5) suggest that there is no strong evidence that being born in a particular quarter affects wages
While the model is statistically significant, it explains only 13.4% of the variation in wages. Other unobserved factors (e.g. ability, skills) likely play a major role."
)
}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
if (show_interpretation) {
cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
if (show_interpretation) {
cat("MKV 1: EDC en AGE zijn stochastic, dummy variables zijn deterministic\n")
}
plot(linear_model1$fitted.values, resid(linear_model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values",
ylab = "Residuals",
pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
# Updated upstream
# Residuen opslaan
residuals <- residuals(linear_model1)
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
labs(title = "Histogram van de Residuen", x = "Residuals", y = "Frequentie") +
theme_minimal()
# Q-Q plot
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)
if (show_interpretation) {
cat("Since the p value is small, we reject the null hypothesis .Therefore the residuals are NOT normally distributed.
Implications for OLS:
1) Unbiasedness: Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity,        exogeneity of regressors, and zero-mean error term) are satisfied
2) Efficiency:
OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator  (BLUE) under the Gauss-Markov assumptions.
3) Hypothesis Testing:
The validity of tests (e.g., (t)-tests and (F)-tests) and confidence intervals relies on the normality assumption. Non-normal residuals         can lead to incorrect p-values
4) Heteroscedasticity or Outliers:
Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.
Remediation:
Based on the histogram and Q-Q Plot which is slightly right skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.
"
)}
if (show_interpretation) {
cat("Gauss-Markov assumptions:
\n Assumption 1: Linearity in the parameters: CHECK
\n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
\n Correlation between explanatory variables and error terms:
")
df$residuals = residuals
cor(df[, c("residuals", "EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5", "REGION6",
"REGION7", "REGION8", "REGION9", "QOB2", "QOB3", "QOB4")])
}
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
if (isTRUE(show_interpretation)) {
cat("### **1.Hypothesis Testing**\n\n")  # Adds a Markdown header for clarity
cat("**H0:** B2 = 0 → Education has no effect on wages.\n\n")
cat("**H1:** B2 > 0 → Education has a positive effect on wages.\n\n")
cat("One-sided hypotheses are appropriate in this case because education is expected to increase earnings (according to economic theories).
In addition, there’s often little reason to test whether more education reduces wages because that result is counterintuitive.")
}
cat("### **2. Explore the Data**\n\n")
head(df)
summary(df)
describe(df)
cov(df)
cor(df)
#Creation of dummy vars for the regions and QOBs
df <- df %>%
mutate(
REGION1 = as.integer(REGION == 1),
REGION2 = as.integer(REGION == 2),
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),
)
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
geom_point(color = "blue") +
labs(title = "Relationship Between Education and Wage",
x = "Years of Education",
y = "Weekly Wage (in $)") + ylim(0, 12000) +
theme_minimal()
cat("### **3.Estimate the baseline Specification**\n\n")
#linear model
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9, data = df)
stargazer(linear_model1,type="text",style="all")
if (show_interpretation) {
cat("### Interpretation of the estimated Coefficient \n\n")
cat("\n EDUC (26.729, p < 0.001)
A one-year increase in education leads to a $26.73 increase in wages.
This is highly significant (p = 0.000).
AGE (2.226, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).
RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).
SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.
MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).
Regional Effects on Wages
Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.
Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.
QOB:
being born in Q2 is associated with a $3.94 lower wage compared to Q1,
Being born in Q3 is associated with a $4.79 higher wage compared to Q1, while Being born in Q4 is associated with a $3.73 lower wage compared to Q1.None of the QOB coefficients are significant, meaning quarter of birth does not have a meaningful impact on wages in this model.
The p-values (all > 0.5) suggest that there is no strong evidence that being born in a particular quarter affects wages
While the model is statistically significant, it explains only 13.4% of the variation in wages. Other unobserved factors (e.g. ability, skills) likely play a major role."
)
}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
if (show_interpretation) {
cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
if (show_interpretation) {
cat("MKV 1: EDC en AGE zijn stochastic, dummy variables zijn deterministic\n")
}
plot(linear_model1$fitted.values, resid(linear_model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values",
ylab = "Residuals",
pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
# Updated upstream
# Residuen opslaan
residuals <- residuals(linear_model1)
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
labs(title = "Histogram van de Residuen", x = "Residuals", y = "Frequentie") +
theme_minimal()
# Q-Q plot
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)
if (show_interpretation) {
cat("Since the p value is small, we reject the null hypothesis .Therefore the residuals are NOT normally distributed.
Implications for OLS:
1) Unbiasedness: Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity,        exogeneity of regressors, and zero-mean error term) are satisfied
2) Efficiency:
OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator  (BLUE) under the Gauss-Markov assumptions.
3) Hypothesis Testing:
The validity of tests (e.g., (t)-tests and (F)-tests) and confidence intervals relies on the normality assumption. Non-normal residuals         can lead to incorrect p-values
4) Heteroscedasticity or Outliers:
Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.
Remediation:
Based on the histogram and Q-Q Plot which is slightly right skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.
"
)}
if (show_interpretation) {
cat("Gauss-Markov assumptions:
\n Assumption 1: Linearity in the parameters: CHECK
\n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
\n Correlation between explanatory variables and error terms:
")
df$residuals = residuals
cor(df[, c("residuals", "EDUC", "AGE", "RACE", "SMSA", "MARRIED",
"REGION2", "REGION3", "REGION4", "REGION5", "REGION6",
"REGION7", "REGION8", "REGION9")])
}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
cat("Assumption 3: The expected V value of the error terms is zero: CHECK")
}
bptest(linear_model1)
if (show_interpretation) {
cat(" euuhhh , deze test zegt dat er geen heteroskedasticity is , maar onze residuals zijn wel niet nrml verdeeld lol . geen idee hoe ik verder moet ")
}
