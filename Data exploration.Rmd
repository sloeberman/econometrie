---
title: "Case Solution 1 Group 5"
author: "Balint Keller, Ling-Fei Chen , Elisabeth Bonte, Lieke vanhaverbeke, Stef Desender, Seppe Van Campe"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
df_print: paged
---

```{r setup, include=FALSE}
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
library(purrr)

# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
df <- df %>%# update de dataset , nu met categorical var
  mutate(REGION = factor(REGION, 
                         levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9), 
                         labels = c("New England", "Middle Atlantic", "East North Central", "West North Central","South Atlantic","East South     Central","West South Central","Mountain","Pacific ")))
```

```{r hypothesis_test, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **1.Hypothesis Testing**\n\n")  # Adds a Markdown header for clarity
  
  cat("**H0:** B2 ≤ 0 → Education has no effect on wages.\n\n")
  cat("**H1:** B2 > 0 → Education has a positive effect on wages.\n\n")
  
  cat("One-sided hypotheses are appropriate in this case because education is expected to increase earnings (according to economic theories).  
  In addition, there’s often little reason to test whether more education reduces wages because that result is counterintuitive.")
}
```

```{r echo=FALSE, results='asis'}
cat("### **2. Explore the Data**\n\n")
```


```{r head, echo=TRUE}


head(df)
```

```{r summary, echo=TRUE}
summary(df)
```


```{r describe, echo=TRUE}
describe(df)
```

```{r cov, echo=TRUE}
cov(df)
```

```{r statisctics per region, echo=TRUE}
library(dplyr)
library(psych)

<<<<<<< HEAD
df %>%
  group_by(REGION) %>%
  summarize(
    mean_wage = mean(WAGE, na.rm = TRUE),
    mean_educ = mean(EDUC, na.rm = TRUE),
    mean_age = mean(AGE, na.rm = TRUE),
    
    sd_wage = sd(WAGE, na.rm = TRUE),
    sd_educ = sd(EDUC, na.rm = TRUE),
    sd_age = sd(AGE, na.rm = TRUE),
    
    var_wage = var(WAGE, na.rm = TRUE),
    var_educ = var(EDUC, na.rm = TRUE),
    var_age = var(AGE, na.rm = TRUE),
    
    
    count = n()
=======
```{r dummies, echo=TRUE}
#Creation of dummy vars for the regions and QOBs
df <- df %>%
  mutate(
    REGION1 = as.integer(REGION == 1),
    REGION2 = as.integer(REGION == 2),
    REGION3 = as.integer(REGION == 3),
    REGION4 = as.integer(REGION == 4),
    REGION5 = as.integer(REGION == 5),
    REGION6 = as.integer(REGION == 6),
    REGION7 = as.integer(REGION == 7),
    REGION8 = as.integer(REGION == 8),
    REGION9 = as.integer(REGION == 9),

>>>>>>> 770d2de51b92fcc682d9ca47cd91898f4d0abd2a
  )
```


```{r cov per regio, echo=TRUE , results='asis'}
cat(" ###cov matrices per regio")
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    cat("\n## Covariance Matrix for REGION:", region_name, "\n\n")  # Now correctly displays region name!
    print(cov(select_if(.x, is.numeric), use = "pairwise.complete.obs"))  # Compute and display covariance
    cat("\n-----------------------------------------------------\n")
  })



```
```{r cor per regio, echo=TRUE , results='asis'}
cat(" ###cor matrices per regio")
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    cat("\n## Correlation Matrix for REGION:", region_name, "\n\n")  # Now correctly displays region name!
    print(cor(select_if(.x, is.numeric), use = "pairwise.complete.obs"))  # Compute and display covariance
    cat("\n-----------------------------------------------------\n")
  })

```








```{r plot, echo=TRUE}
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
  geom_point(color = "blue") +
  labs(title = "Relationship Between Education and Wage",
       x = "Years of Education",
<<<<<<< HEAD
       y = "Weekly Wage (in $)")+
=======
       y = "Weekly Wage (in $)") + ylim(0, 12000) +
>>>>>>> 770d2de51b92fcc682d9ca47cd91898f4d0abd2a

  theme_minimal()

```

```{r echo=FALSE, results='asis'}
cat("### **3.Estimate the baseline Specification**\n\n")
```

```{r, echo=TRUE}
#linear model

<<<<<<< HEAD
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  , data = df)
=======
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9, data = df)
>>>>>>> 770d2de51b92fcc682d9ca47cd91898f4d0abd2a
stargazer(linear_model1,type="text",style="all")
```


```{r betekenins coef, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("### Interpretation of the estimated Coefficient \n\n")
  cat("\n EDUC (26.696, p < 0.001)

A one-year increase in education leads to a $26.696 increase in weelky wages.
This is highly significant (p = 0.000).

AGE ( 2.244, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).

RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).

SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.

MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).

Regional Effects on Wages

Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.

Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.

<<<<<<< HEAD
")
=======
QOB:
being born in Q2 is associated with a $3.94 lower wage compared to Q1,
Being born in Q3 is associated with a $4.79 higher wage compared to Q1, while Being born in Q4 is associated with a $3.73 lower wage compared to Q1.None of the QOB coefficients are significant, meaning quarter of birth does not have a meaningful impact on wages in this model.
The p-values (all > 0.5) suggest that there is no strong evidence that being born in a particular quarter affects wages

While the model is statistically significant, it explains only 13.4% of the variation in wages. Other unobserved factors (e.g. ability, skills) likely play a major role."
)
>>>>>>> 770d2de51b92fcc682d9ca47cd91898f4d0abd2a
}

```

```{r Joint significance tests}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))

```
```{r Regional differences}
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
```

```{r interpretatie van joint significance, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
```

```{r stochastic of deterministic, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("MKV 1: EDC en AGE zijn stochastic, dummy variables zijn deterministic\n")
}
```

```{r}
plot(linear_model1$fitted.values, resid(linear_model1),
     main = "Residuals vs. Fitted Values",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
```





```{r}

# Updated upstream


# Residuen opslaan
residuals <- residuals(linear_model1) 
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
  geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
  labs(title = "Histogram van de Residuen", x = "Residuals", y = "Frequentie") +
  theme_minimal()
```
```{r}
# Q-Q plot
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)

```

```{r, echo=FALSE, results="asis"}
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)

if (show_interpretation) {
  cat("Since the p value is small, we reject the null hypothesis .Therefore the residuals are NOT normally distributed.
      Implications for OLS:
      1) Unbiasedness: Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity,        exogeneity of regressors, and zero-mean error term) are satisfied
      
      2) Efficiency:
      OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator  (BLUE) under the Gauss-Markov assumptions.
      
      3) Hypothesis Testing:
      The validity of tests (e.g., (t)-tests and (F)-tests) and confidence intervals relies on the normality assumption. Non-normal residuals         can lead to incorrect p-values
      
      4) Heteroscedasticity or Outliers:
      Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias. 
      
      Remediation:
      Based on the histogram and Q-Q Plot which is slightly right skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.
      
      
      "
)}
```

```{r, echo=FALSE, results="asis"}

if (show_interpretation) {
  cat("Gauss-Markov assumptions:
  \n Assumption 1: Linearity in the parameters: CHECK
  \n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
  \n Correlation between explanatory variables and error terms:
  ")
  df$residuals = residuals
  cor(df[, c("residuals", "EDUC", "AGE", "RACE", "SMSA", "MARRIED",
           "REGION2", "REGION3", "REGION4", "REGION5", "REGION6",
           "REGION7", "REGION8", "REGION9")])
  
}
```


```{r}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
  cat("Assumption 3: The expected V value of the error terms is zero: CHECK")
}
```




```{r testen op heteroskedasticity via Breusch-Pagan }
bptest(linear_model1)

```
```{r Breusch pagan interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" geen heteorskedasticity")
}
```


