---
title: "Case Solution Group 5"
author: "Balint Keller, Ling-Fei Chen , Elisabeth Bonte, Lieke Vanhaverbeke, Stef
  Desender, Seppe Van Campe"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
df_print: paged
---

```{r setup, include=FALSE}
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
library(purrr)
library(corrplot)
library(randtests)

# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
df <- df %>% mutate(
REGION1 = as.integer(REGION == 1), 
REGION2 = as.integer(REGION == 2), 
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),

QOB1 = as.integer(QOB == 1),
QOB2 = as.integer(QOB == 2),
QOB3 = as.integer(QOB == 3),
QOB4 = as.integer(QOB == 4)
)
```

```{r hypothesis_test, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **1. Empirical Specification and Hypotheses**\n\n")

cat("Baseline model:\n\n")
cat("$$WAGE_i = \\beta_1 + \\beta_2 \\, EDUCi + \\alpha \\, X_i + \\mu_i$$\n\n")
cat("Where Xi includes AGE, RACE, MARRIED, SMSA, and REGION.\n\n")

cat("### **Hypothesis Tests:**\n\n")

cat("EDUC: Education\n\n")
cat("H0: B2 $\\leq$ 0 → Education has no or a negative effect on wages.\n\n")
cat("H1: B2 $>$ 0 → Education has a positive effect on wages.\n\n")
cat("One-sided test: Economic theory predicts that more education increases wages, so a negative effect would be counterintuitive.\n\n")

cat("AGE: Age\n\n")
cat("H0: A(AGE) $\\leq$ 0 → Age has no or a negative effect on wages.\n\n")
cat("H1: A(AGE) $>$ 0 → Age has a positive effect on wages.\n\n")
cat("One-sided test: Age is often associated with experience, which is expected to increase wages.\n\n")

cat("MARRIED: Marital Status\n\n")
cat("H0: A(MARRIED) $\\leq$ 0 → Being married has no or a negative effect on wages.\n\n")
cat("H1: A(MARRIED) $>$ 0 → Being married has a positive effect on wages.\n\n")
cat("One-sided test: Being married may indicate stability and a positive impact on productivity, leading to higher wages.\n\n")

cat("SMSA: Metropolitan Area\n\n")
cat("H0: A(SMSA) $\\leq$ 0 → Living in a metropolitan area has no or a negative effect on wages.\n\n")
cat("H1: A(SMSA) $>$ 0 → Living in a metropolitan area has a positive effect on wages.\n\n")
cat("One-sided test: Urban areas typically have more job opportunities and higher wages, driving this expectation.\n\n")

cat("RACE: Race (1 = Black, 0 = White)\n\n")
cat("H0: A(RACE) $\\geq$ 0 → Race has no or a positive effect on wages.\n\n")
cat("H1: A(RACE) $<$ 0 → Race has a negative effect on wages.\n\n")
cat("One-sided test: Structural inequalities often result in lower wages for minority groups.\n\n")

cat("REGION: Geographic Region\n\n")
cat("H0: A(REGION) $\\neq$ 0 → (There are no positive wage differences across regions.)\n\n")
cat("H1: A(REGION) $=$ 0 → (At least one region has a positive effect on wages.)\n\n")
cat("Two-sided test: Region could impact wages in either direction\n\n")

}
```

```{r header 2, echo=FALSE, results='asis'}
cat("### **2. Explore the Data**\n\n")
```


```{r head, echo=TRUE}
head(df)
```

```{r summary, echo=TRUE}
summary(df)
```


```{r describe, echo=TRUE}
describe(df)
```

```{r cov, echo=TRUE}
cov(df)
```
```{r statisctics per region, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **Statistics per region**\n\n")  # Adds a Markdown header for clarity
  cat("WAGE: Largest wage disparities in regions 2, 6, and 9; high SD, skewness, and kurtosis → outliers with very high incomes.\n\n")
  cat("EDUC: Highest in region 9, lowest in region 6; slightly more variation in regions 5 and 6.\n\n")
  cat("AGE: Almost uniformly ~45 years across regions; low spread, minimal differences.\n\n")
  cat("RACE: Mostly white population (>90%); region 5 relatively more diversity; high kurtosis indicates outliers.\n\n")
  cat("SMSA: Regions 4 & 6 are more urban, region 2 is the most rural; skewed → most live outside urban areas.\n\n")
  cat("MARRIED: High marriage rates across all regions (~85–89%); region 9 slightly lower.\n\n")
}
```

```{r statisctics per region code, echo=FALSE}
library(dplyr)
library(psych)


df %>%
  group_by(REGION) %>%
  summarize(
    mean_wage = mean(WAGE, na.rm = TRUE),
    mean_educ = mean(EDUC, na.rm = TRUE),
    mean_age = mean(AGE, na.rm = TRUE),
    
    sd_wage = sd(WAGE, na.rm = TRUE),
    sd_educ = sd(EDUC, na.rm = TRUE),
    sd_age = sd(AGE, na.rm = TRUE),
    
    var_wage = var(WAGE, na.rm = TRUE),
    var_educ = var(EDUC, na.rm = TRUE),
    var_age = var(AGE, na.rm = TRUE),
    
    
    count = n()
  )

#dit is hoe polina het toont in feedback (ook zelfde uitkomsten als haar)
#alleen de relevante kolommen (alle numerieke behalve dummies)
numeric_vars <- df %>% 
  select_if(is.numeric) %>%
  select(-starts_with("REGION"), -starts_with("QOB"))  # dummies eruit

#combineer met regio
df_subset1 <- df %>% 
  select(REGION) %>% 
  bind_cols(numeric_vars)

#beschrijvende statistieken per regio
describeBy(df_subset1, group = "REGION")
```



```{r statisctics per age, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **Statistics per age**\n\n")
  cat("AGE GROUPS:\n\n")
  cat("AGE >= 40 & AGE < 42 ~ '40-41 GROUP 1',\n\n")
  cat("AGE >= 42 & AGE < 44 ~ '42-43 GROUP 2',\n\n")
  cat("AGE >= 44 & AGE < 46 ~ '44-45 GROUP 3',\n\n")
  cat("AGE >= 46 & AGE < 48 ~ '46-47 GROUP 4',\n\n")
  cat("AGE >= 48 & AGE <= 50 ~ '48-50 GROUP 5'\n\n")
}
```


```{r statisctics per age code, echo=FALSE}
#dit is voor age groups
df_subset2 <- df %>%
  mutate(AGE_GROUP = case_when(
    AGE >= 40 & AGE < 42 ~ "40-41",
    AGE >= 42 & AGE < 44 ~ "42-43",
    AGE >= 44 & AGE < 46 ~ "44-45",
    AGE >= 46 & AGE < 48 ~ "46-47",
    AGE >= 48 & AGE <= 50 ~ "48-50"
  )) %>%
  select(-starts_with("REGION"), -starts_with("QOB"))

# Beschrijvende statistieken per AGE_GROUP
describeBy(df_subset2, group = "AGE_GROUP")
```


```{r cov per regio, echo=FALSE , results='asis'}
cat("### **Covariance matrices per regio**\n\n")
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    knitr::kable(cov(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED, QOB, QOB1:QOB4), use = "pairwise.complete.obs"), 
             caption = paste("Covariance Matrix for REGION:", region_name),
             digits = 3)
  })

```


```{r cor per regio, echo=FALSE , results='asis'}
cat("### **Correlation matrices per regio**\n\n")
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    knitr::kable(cor(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED, QOB, QOB1:QOB4),  use = "pairwise.complete.obs"),
             caption = paste("Correlation Matrix for REGION:", region_name),
             digits = 3)
  })

```

```{r plot, echo=FALSE}
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
  geom_point(color = "blue") +
  labs(title = "Relationship Between Education and Wage",
       x = "Years of Education",
       y = "Weekly Wage (in $)")+

  theme_minimal()

```

```{r header 3, echo=FALSE, results='asis'}
cat("### **3.Estimate the baseline Specification**\n\n")
```

```{r linear model, echo=TRUE}
#linear model
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  , data = df)
```

```{r stargazer lin model, echo=FALSE}
stargazer(linear_model1,type="text",style="all")
```


```{r betekenins coef, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("### Interpretation of the estimated Coefficient \n\n")
  cat("\n EDUC (26.696, p < 0.001)

A one-year increase in education leads to a $26.696 increase in weelky wages.
This is highly significant (p = 0.000).

AGE ( 2.244, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).

RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).

SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.

MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).

Regional Effects on Wages

Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.

Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.
")
}

```

```{r Joint significance tests, echo=FALSE}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))

```

```{r interpretatie joint significance tests, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("The joint significance test shows that AGE, RACE, MARRIED, and SMSA jointly contribute significantly to explaining WAGE.\n",
    "The F-statistic is 60.23 with a p-value < 2.2e-16, indicating strong statistical significance.\n",
    "We reject the null hypothesis that these four variables have no joint effect on wages.\n")
}
```

```{r Regional differences, echo=FALSE}
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
```

```{r interpretatie van joint significance, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
```

```{r header 4, echo=FALSE, results="asis"}
cat("### **4. Evaluating Gauss-Markov Assumptions and Applying Remedial Measures**\n\n")
cat("#### **4a. Stochastic Regressors**\n\n")

```


```{r stochastic of deterministic, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("All the variables are stochastic.\n\n")
  cat("
Strictly exogenous: AGE, RACE, QOB
- These variables are fixed and not influenced by wages or the error term.

Weakly exogenous: SMSA, REGION
- These variables may be correlated with unobserved factors affecting wages, but are not directly influenced by wages.

Endogenous: EDUC, MARRIED
- Potential for reverse causality or correlation with the error term (e.g., higher wages → more education or higher likelihood of marriage).
")
}
```

```{r residuals plot, echo=FALSE}
plot(linear_model1$fitted.values, resid(linear_model1),
     main = "Residuals vs. Fitted Values",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
```



```{r histogram plot, echo=FALSE}

# Updated upstream


# Residuen opslaan
residuals <- residuals(linear_model1) 
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
  geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
  labs(title = "Histogram Residuals", x = "Residuals", y = "Frequentie") +
  theme_minimal()
```

```{r header 4b, echo=FALSE, results="asis"}
cat("#### **4b. Normality Error Terms**\n\n")
```

```{r QQ plot, echo=FALSE}
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
```

```{r QQ plot_layout, echo=FALSE, fig.width=8, fig.height=6}
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
```

```{r jarque bera test_layout, echo=FALSE, results="asis"}
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)

if (show_interpretation) {
  cat(paste(
    "Since the p value is small, we reject the null hypothesis. Therefore the residuals are NOT normally distributed.",
    "",
    "Implications for OLS:",
    "",
    "1) Unbiasedness:",
    "Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity, exogeneity of regressors, and zero-mean error term) are satisfied.",
    "",
    "2) Efficiency:",
    "OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov assumptions.",
    "",
    "3) Hypothesis Testing:",
    "The validity of tests (e.g., t-tests and F-tests) and confidence intervals relies on the normality assumption. Non-normal residuals can lead to incorrect p-values.",
    "",
    "4) Heteroscedasticity or Outliers:",
    "Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.",
    "",
    "Remediation:",
    "Based on the histogram and Q-Q Plot which is slightly right-skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.",
    sep = "\n"
  ))
}
```

```{r jarque bera test, echo=FALSE, results="asis"}
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)

if (show_interpretation) {
  cat("Since the p value is small, we reject the null hypothesis .Therefore the residuals are NOT normally distributed.
  
      Implications for OLS:
      
      1) Unbiasedness: Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity,        exogeneity of regressors, and zero-mean error term) are satisfied
      
      2) Efficiency:
      OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator  (BLUE) under the Gauss-Markov assumptions.
      
      3) Hypothesis Testing:
      The validity of tests (e.g., (t)-tests and (F)-tests) and confidence intervals relies on the normality assumption. Non-normal residuals         can lead to incorrect p-values
      
      4) Heteroscedasticity or Outliers:
      Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias. 
      
      Remediation:Based on the histogram and Q-Q Plot which is slightly right skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies."
)}
```


```{r GM assumptrions, echo=FALSE, results="asis"}

if (show_interpretation) {
  cat("Gauss-Markov assumptions:
  \n Assumption 1: Linearity in the parameters: CHECK
  \n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
  \n Correlation between explanatory variables and error terms:
  ")
  
}
```


```{r assumption 3}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
  cat("Assumption 3: The expected V value of the error terms is zero: CHECK")
}
```
```{r Heterskedasticity, echo=FALSE, results='asis'}
cat("### **4(d) Heteroskedasticity**\n\n")
```

```{r residuals_plot, echo=FALSE}
squared_res <- resid(linear_model1)^2
plot(linear_model1$fitted.values, squared_res,
     main = "Squared residuals vs. Fitted Values",
     xlab = "Fitted Values",
     ylab = "Squared residuals",
     pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero

explanatory_vars <- names(linear_model1$model)[-1]  # exclude response variable

# Plot squared residuals vs each predictor
par(mar = c(4, 4, 1, 1))  # Set smaller margins
# Adjust layout if needed
for (var in explanatory_vars) {
  plot(linear_model1$model[[var]], squared_res,
       main = paste("Squared Residuals vs", var),
       xlab = var,
       ylab = "Squared Residuals",
       pch = 16, col = "black")
  abline(h = 0, lty = 2, col = "red")
}


```
```{r uitleg_plots echo=False ,results='asis'}
if (show_interpretation) {
  cat("Fitted values 
        we see a slight increase in the spread , when the fitted values increase, what a sign of heteroskedasticity isµ
      EDC
        for EDC we see first a slight increase and then a decrease, which again is a pattern , so again a sign of heteroskedasticity
      RACE
        we see a bigger spread for white people
      SMSA
        not living in a metroploarian place has a bigger spread
      MARRIED
        same
      REGION
         for every region we see a different spread between 0 and          1
      CONCLUSION
      we see clear signs of heteroskedasticity in the plots")}

```
```{r Heteroskedasticity_test, echo=FALSE, results='asis'}
cat("Heteroskedasticity test ")
```



```{r White_test  echo=True , results="asis"}
white_test <- bptest(linear_model1, ~ fitted(linear_model1) + I(fitted(linear_model1)^2))

# Output result
print(white_test)

```
```{r White_ interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" the p value of 0.02977 says that we  reject the null hyptothis , which implies that we reject homoskedastiscity
      
      for white there are not properties")
}
```

```{r QF_test  echo=True , results="asis"}
gqtest_result <- gqtest(linear_model1)

# Output result
print(gqtest_result)

```


```{r GF_ interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" the p value of p-value =
5.067e-09 says that we strongly reject the null hyptothis , which implies that we reject homoskedastiscity

properties
   You split the dataset into two non-overlapping groups (dropping middle observations).

Errors are normally distributed.")
}


```

```{r Breusch pagan interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("Multicollinearity\n
      Gauss-Markov assumption (no perfect multicollinearity) is met.
      \n\n
      Analysis of Variance Inflation Factors and correlation:
      Low VIFs (<5): Most of the variables have VIF values close to 1, including EDUC (1.07), AGE (1.01), RACE (1.05), SMSA (1.07), and MARRIED (1.02). These values suggest minimal multicollinearity, meaning these predictors are relatively independent.\n
      
      Moderate VIFs (Between 2 and 5): Some regional variables—REGION2 (3.28), REGION3 (3.67), REGION4 (2.18), REGION5 (3.51), REGION6 (2.06), REGION7 (2.54), REGION8 (1.82), and REGION9 (2.92)—show moderate correlation with other predictors. These values indicate some degree of collinearity, but they are not alarmingly high.
      \n
      High VIFs (>5): None of the variables exceed 5, which suggests that severe multicollinearity is not a major issue in this regression model. But even if VIF would be high (VIF>5), a large sample (n = 10 000 in this case) and high variance
in the explanatory variables can still lead to precise estimates.



      Implications for the properties and precision of the OLS estimator:
      1) Parameters remain identifiable.\n
      2) Under the CNLRM assumptions, the OLS estimator remains BLUE and normally distributed.\n
      3) OLS estimators exhibit larger variance and covariance, especially the regions.\n
      4) Wider confidence intervals and lower t-statistics 
      ->variables appear less significantly different from zero, higher probability of making a Type II error\n
      5) The overall model fit (R2) is largely unaffected: even if individual coefficients are insignificant, the F -test may
indicate that the coefficients are jointly significant, and can be estimated with high precision.\n
      6) Regression coefficients may change substantially\n
      ")



correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED", 
                                 "REGION2", "REGION3", "REGION4", "REGION5", 
                                 "REGION6", "REGION7", "REGION8", "REGION9")])


corrplot(correlation_matrix, method = "circle")

}

```

```{r}
print(correlation_matrix)
print(max(abs(correlation_matrix)))
```
```{r}
print('VIF values')
vif_values = vif(linear_model1)
print(vif_values)
```


```{r autocorrelation graph}
df$residuals <- residuals
df_ordered <- df[order(df$EDUC, df$AGE), ]
plot(df_ordered$residuals, type = "l",
     main = "Residuals Ordered by EDUC and AGE",
     ylab = "Residuals", xlab = "Observation Index")
```

```{r autocorrelation runs test}
runs.test(df$residuals)
if (show_interpretation) {
  cat("Assumptions:
  1) no need to assume specific error structure
  2) The observations should be in a meaningful order as the test evaluates the sequence
  High p-value suggest that there is no autocorrelation.")
  }
```

```{r autocorrelation DW test}
DW=dwtest(linear_model1)
# print to screen
DW_summary=c(DW$statistic,DW$p.value)
names(DW_summary)=c("Test-statistic","P-value")
stargazer(DW_summary,type="text")
if (show_interpretation) {
  cat("Assumptions:
  1) Error terms follow an AR(1) process
  2) Error terms are normally distributed
  3) The regression model does not include lagged dependent variables
  High p-value suggest that there is no autocorrelation.")
  }
```

```{r autocorrelation BG test}
BG = bgtest(linear_model1, order = 6)  # Change order as needed
BG_summary=c(BG$statistic,BG$p.value)
names(BG_summary)=c("Test-statistic","P-value")
stargazer(BG_summary,type="text")
if (show_interpretation) {
  cat("Assumptions:
  1) Allows for stochastic regressors
  2) Tests for higher-order autocorrelation
  3) Allows for lagged dependent variables
  High p-value suggest that there is no autocorrelation.")
  }
```

