---
title: "Case Solution Group 5"
author: "Balint Keller, Ling-Fei Chen , Elisabeth Bonte, Lieke Vanhaverbeke, Stef
  Desender, Seppe Van Campe"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
df_print: paged
---

```{r setup, include=FALSE}
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <-TRUE
#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
library(purrr)
library(corrplot)
library(randtests)
library(sandwich)
library(nlme)


# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
df <- df %>% mutate(
REGION1 = as.integer(REGION == 1), 
REGION2 = as.integer(REGION == 2), 
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),

QOB1 = as.integer(QOB == 1),
QOB2 = as.integer(QOB == 2),
QOB3 = as.integer(QOB == 3),
QOB4 = as.integer(QOB == 4)
)
```

```{r hypothesis_test, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("# **1. Empirical Specification and Hypotheses**\n\n")

cat("Baseline model:\n\n")
cat("$$WAGE_i = \\beta_1 + \\beta_2 \\, EDUCi + \\alpha \\, X_i + \\mu_i$$\n\n")
cat("Where Xi includes AGE, RACE, MARRIED, SMSA, and REGION.\n\n")

cat("## **Hypothesis Tests:**\n\n")

cat("##### **EDUC: Education**\n\n")
cat("H0: B2 $\\leq$ 0 → Education has no or a negative effect on wages.\n\n")
cat("H1: B2 $>$ 0 → Education has a positive effect on wages.\n\n")
cat("One-sided test: Economic theory predicts that more education increases wages, so a negative effect would be counterintuitive.\n\n")

cat("##### **AGE: Age**\n\n")
cat("H0: A(AGE) $\\leq$ 0 → Age has no or a negative effect on wages.\n\n")
cat("H1: A(AGE) $>$ 0 → Age has a positive effect on wages.\n\n")
cat("One-sided test: Age is often associated with experience, which is expected to increase wages.\n\n")

cat("##### **MARRIED: Marital Status**\n\n")
cat("H0: A(MARRIED) $\\leq$ 0 → Being married has no or a negative effect on wages.\n\n")
cat("H1: A(MARRIED) $>$ 0 → Being married has a positive effect on wages.\n\n")
cat("One-sided test: Being married may indicate stability and a positive impact on productivity, leading to higher wages.\n\n")

cat("##### **SMSA: Metropolitan Area**\n\n")
cat("H0: A(SMSA) $\\leq$ 0 → Living in a metropolitan area has no or a negative effect on wages.\n\n")
cat("H1: A(SMSA) $>$ 0 → Living in a metropolitan area has a positive effect on wages.\n\n")
cat("One-sided test: Urban areas typically have more job opportunities and higher wages, driving this expectation.\n\n")

cat("##### **RACE: Race (1 = Black, 0 = White)**\n\n")
cat("H0: A(RACE) $\\geq$ 0 → Race has no or a positive effect on wages.\n\n")
cat("H1: A(RACE) $<$ 0 → Race has a negative effect on wages.\n\n")
cat("One-sided test: Structural inequalities often result in lower wages for minority groups.\n\n")

cat("##### **REGION: Geographic Region**\n\n")
cat("H0: A(REGION) $\\neq$ 0 → (There are no positive wage differences across regions.)\n\n")
cat("H1: A(REGION) $=$ 0 → (At least one region has a positive effect on wages.)\n\n")
cat("Two-sided test: Region could impact wages in either direction\n\n")

}
```

```{r header 2, echo=FALSE, results='asis'}
cat("# **2. Explore the Data**\n\n")
```


```{r head, echo=TRUE}
head(df)
```

```{r summary, echo=TRUE}
summary(df)
```


```{r describe, echo=TRUE}
describe(df)
```

```{r cov, echo=TRUE}
cov(df)
```
```{r statisctics per region, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **2.1 Statistics per region**\n\n")  # Adds a Markdown header for clarity
  cat("WAGE: Largest wage disparities in regions 2, 6, and 9; high SD, skewness, and kurtosis → outliers with very high incomes.\n\n")
  cat("EDUC: Highest in region 9, lowest in region 6; slightly more variation in regions 5 and 6.\n\n")
  cat("AGE: Almost uniformly ~45 years across regions; low spread, minimal differences.\n\n")
  cat("RACE: Mostly white population (>90%); region 5 relatively more diversity; high kurtosis indicates outliers.\n\n")
  cat("SMSA: Regions 4 & 6 are more urban, region 2 is the most rural; skewed → most live outside urban areas.\n\n")
  cat("MARRIED: High marriage rates across all regions (~85–89%); region 9 slightly lower.\n\n")
}
```

```{r statisctics per region code, echo=FALSE}
library(dplyr)
library(psych)


df %>%
  group_by(REGION) %>%
  summarize(
    mean_wage = mean(WAGE, na.rm = TRUE),
    mean_educ = mean(EDUC, na.rm = TRUE),
    mean_age = mean(AGE, na.rm = TRUE),
    
    sd_wage = sd(WAGE, na.rm = TRUE),
    sd_educ = sd(EDUC, na.rm = TRUE),
    sd_age = sd(AGE, na.rm = TRUE),
    
    var_wage = var(WAGE, na.rm = TRUE),
    var_educ = var(EDUC, na.rm = TRUE),
    var_age = var(AGE, na.rm = TRUE),
    
    
    count = n()
  )

#dit is hoe polina het toont in feedback (ook zelfde uitkomsten als haar)
#alleen de relevante kolommen (alle numerieke behalve dummies)
numeric_vars <- df %>% 
  select_if(is.numeric) %>%
  select(-starts_with("REGION"), -starts_with("QOB"))  # dummies eruit

#combineer met regio
df_subset1 <- df %>% 
  select(REGION) %>% 
  bind_cols(numeric_vars)

#beschrijvende statistieken per regio
describeBy(df_subset1, group = "REGION")
```



```{r statisctics per age, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **2.2 Statistics per age**\n\n")
  cat("AGE GROUPS:\n\n")
  cat("AGE >= 40 & AGE < 42 ~ '40-41 GROUP 1',\n\n")
  cat("AGE >= 42 & AGE < 44 ~ '42-43 GROUP 2',\n\n")
  cat("AGE >= 44 & AGE < 46 ~ '44-45 GROUP 3',\n\n")
  cat("AGE >= 46 & AGE < 48 ~ '46-47 GROUP 4',\n\n")
  cat("AGE >= 48 & AGE <= 50 ~ '48-50 GROUP 5'\n\n")
}
```


```{r statisctics per age code, echo=FALSE}
#dit is voor age groups
df_subset2 <- df %>%
  mutate(AGE_GROUP = case_when(
    AGE >= 40 & AGE < 42 ~ "40-41",
    AGE >= 42 & AGE < 44 ~ "42-43",
    AGE >= 44 & AGE < 46 ~ "44-45",
    AGE >= 46 & AGE < 48 ~ "46-47",
    AGE >= 48 & AGE <= 50 ~ "48-50"
  )) %>%
  select(-starts_with("REGION"), -starts_with("QOB"))

# Beschrijvende statistieken per AGE_GROUP
describeBy(df_subset2, group = "AGE_GROUP")
```


```{r cov per regio, echo=FALSE , results='asis'}
cat("### **2.3 Covariance matrices per regio**\n\n")
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    knitr::kable(cov(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED, QOB, QOB1:QOB4), use = "pairwise.complete.obs"), 
             caption = paste("Covariance Matrix for REGION:", region_name),
             digits = 3)
  })

```


```{r cor per regio, echo=FALSE , results='asis'}
cat("### **2.4 Correlation matrices per regio**\n\n")
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    knitr::kable(cor(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED, QOB, QOB1:QOB4),  use = "pairwise.complete.obs"),
             caption = paste("Correlation Matrix for REGION:", region_name),
             digits = 3)
  })

```

```{r plot, echo=FALSE}
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
  geom_point(color = "blue") +
  labs(title = "Relationship Between Education and Wage",
       x = "Years of Education",
       y = "Weekly Wage (in $)")+

  theme_minimal()

```

```{r header 3, echo=FALSE, results='asis'}
cat("# **3.Estimate the baseline Specification**\n\n")
```

```{r linear model, echo=TRUE}
#linear model
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  , data = df)
```

```{r stargazer lin model, echo=FALSE}
stargazer(linear_model1,type="text",style="all")
```


```{r betekenins coef, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("### **3.1 Interpretation of the estimated Coefficient **\n\n")
  cat("\n EDUC (26.696, p < 0.001)

A one-year increase in education leads to a $26.696 increase in weelky wages.
This is highly significant (p = 0.000).

AGE ( 2.244, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).

RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).

SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.

MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).

Regional Effects on Wages

Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.

Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.
")
}

```

```{r Joint significance tests, echo=FALSE}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))

```

```{r interpretatie joint significance tests, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("The joint significance test shows that AGE, RACE, MARRIED, and SMSA jointly contribute significantly to explaining WAGE.\n",
    "The F-statistic is 60.23 with a p-value < 2.2e-16, indicating strong statistical significance.\n",
    "We reject the null hypothesis that these four variables have no joint effect on wages.\n")
}
```

```{r Regional differences, echo=FALSE}
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
```

```{r interpretatie van joint significance, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
```

```{r header 4, echo=FALSE, results="asis"}
cat("# **4. Evaluating Gauss-Markov Assumptions and Applying Remedial Measures**\n\n")
cat("#### **4a. Stochastic Regressors**\n\n
    
    ")

```


```{r stochastic of deterministic, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("All the variables are stochastic.\n\n")
  cat("
Strictly exogenous: AGE, RACE, QOB

- These variables are fixed and not influenced by wages or the error term.
- Implications for OLS: None
- No remediation needed

Weakly exogenous: SMSA, REGION

- These variables may be correlated with unobserved factors affecting wages, but are not directly influenced by wages.
- Implications for OLS: OLS may suffer from omitted variable bias but remains consistent if no correlation with the error term.
- Possible remediation needed

Endogenous: EDUC, MARRIED

- Potential for reverse causality or correlation with the error term (e.g., higher wages → more education or higher likelihood of marriage).
- Implications for OLS: OLS is biased and inconsistent.
- Remediation needed
")
}
```

```{r header 4b, echo=FALSE, results="asis"}
cat("#### **4b. Normality Error Terms**\n\n
    
    ")
```

```{r residuals plot, echo=FALSE, fig.align='center'}
plot(linear_model1$fitted.values, resid(linear_model1),
     main = "Residuals vs. Fitted Values",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero
```



```{r histogram plot, echo=FALSE}

# Updated upstream


# Residuen opslaan
residuals <- residuals(linear_model1) 
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
  geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
  labs(title = "Histogram Residuals", x = "Residuals", y = "Frequentie") +
  theme_minimal()
```


```{r QQ plot_layout, echo=FALSE, fig.width=8, fig.height=6, fig.align='center'}
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
```

```{r jarque bera test_layout, echo=FALSE, results="asis"}
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)

if (show_interpretation) {
  cat(paste(
    "Since the p value is small, we reject the null hypothesis. Therefore the residuals are NOT normally distributed.",
    "",
    "Implications for OLS:",
    "",
    "1) Unbiasedness:",
    "Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity, exogeneity of regressors, and zero-mean error term) are satisfied.",
    "",
    "2) Efficiency:",
    "OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov assumptions.",
    "",
    "3) Hypothesis Testing:",
    "The validity of tests (e.g., t-tests and F-tests) and confidence intervals relies on the normality assumption. Non-normal residuals can lead to incorrect p-values.",
    "",
    "4) Heteroscedasticity or Outliers:",
    "Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.",
    "",
    "Remediation:",
    "Based on the histogram and Q-Q Plot which is slightly right-skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.",
    sep = "\n"
  ))
}
```


```{r GM assumptrions, echo=FALSE, results="asis"}

if (show_interpretation) {
  cat("#### **Gauss-Markov assumptions**:
  \n#### Assumption 1: Linearity in the parameters: CHECK
  \n#### Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model): FAIL
  \n Correlation between explanatory variables and error terms:
  ")
  
}
```


```{r assumption 3, echo=FALSE, results="asis"}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
  cat("#### Assumption 3: The expected V value of the error terms is zero: CHECK\n\n")
}
```


```{r multicoll, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("#### **4c. Multicollinearity**\n\n")
  cat("
Gauss-Markov assumption (no perfect multicollinearity) is met.
  
Analysis of Variance Inflation Factors and correlation:

Low VIFs (<5): Most of the variables have VIF values close to 1, 
including EDUC (1.07), AGE (1.01), RACE (1.05), SMSA (1.07), and MARRIED (1.02). 
These values suggest minimal multicollinearity, meaning these predictors 
are relatively independent.
      
Moderate VIFs (Between 2 and 5): Some regional variables—REGION2 (3.28), 
REGION3 (3.67), REGION4 (2.18), REGION5 (3.51), REGION6 (2.06), REGION7 (2.54), 
REGION8 (1.82), and REGION9 (2.92)—show moderate correlation with other predictors. 
These values indicate some degree of collinearity, but they are not alarmingly high.

High VIFs (>5): None of the variables exceed 5, which suggests that severe 
multicollinearity is not a major issue in this regression model. 
But even if VIF would be high (VIF>5), a large sample (n = 10 000 in this case) 
and high variance in the explanatory variables can still lead to precise estimates.


Implications for the properties and precision of the OLS estimator:

1) Parameters remain identifiable.

2) Under the CNLRM assumptions, the OLS estimator remains BLUE and normally distributed.

3) OLS estimators exhibit larger variance and covariance, especially the regions.

4) Wider confidence intervals and lower t-statistics -> variables appear less 
significantly different from zero, higher probability of making a Type II error

5) The overall model fit (R2) is largely unaffected: even if individual coefficients
are insignificant, the F -test may indicate that the coefficients are jointly
significant, and can be estimated with high precision.

6) Regression coefficients may change substantially")}
```

```{r correleation matrix en plot, echo=FALSE, results="asis"}
{correlation_matrix <- cor(df[, c("EDUC", "AGE", "RACE", "SMSA", "MARRIED", 
                                 "REGION2", "REGION3", "REGION4", "REGION5", 
                                 "REGION6", "REGION7", "REGION8", "REGION9")])


corrplot(correlation_matrix, method = "circle")

}

```

```{r titel plot, echo=FALSE, results='asis'}
cat("**Correlation matrix**\n\n")
```

```{r print corr, echo=FALSE, results="asis"}
print(correlation_matrix)
print(max(abs(correlation_matrix)))
```

```{r titel VIF, echo=FALSE, results='asis'}
cat("**VIF**\n\n")
```

```{r}
print('VIF values')
vif_values = vif(linear_model1)
print(vif_values)
```


```{r Heteroskedasticity, echo=FALSE, results='asis'}
cat("#### **4d. Heteroskedasticity**\n\n")
```

```{r residuals_plot, echo=FALSE, fig.align='center'}
squared_res <- resid(linear_model1)^2
plot(linear_model1$fitted.values, squared_res,
     main = "Squared residuals vs. Fitted Values",
     xlab = "Fitted Values",
     ylab = "Squared residuals",
     pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero

explanatory_vars <- names(linear_model1$model)[-1]  # exclude response variable
```


```{r residuals_plot predictors, echo=FALSE}
# Plot squared residuals vs each predictor
par(mar = c(4, 4, 1, 1))  # Set smaller margins
# Adjust layout if needed
for (var in explanatory_vars) {
  plot(linear_model1$model[[var]], squared_res,
       main = paste("Squared Residuals vs", var),
       xlab = var,
       ylab = "Squared Residuals",
       pch = 16, col = "black")
  abline(h = 0, lty = 2, col = "red")
}


```

```{r uitleg_plots, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("
### Interpretation of Residual Plots:

1. **Fitted Values**:
   - Slight increase in spread as fitted values increase
   - This pattern indicates potential heteroskedasticity

2. **EDUC**:
   - Initial increase followed by decrease in spread
   - Non-constant variance pattern visible

3. **RACE**:
   - Larger spread observed for white participants
   - Unequal variance between groups

4. **SMSA**:
   - Non-metropolitan areas show greater spread
   - Variance differs by urban/rural classification

5. **MARRIED**:
   - Similar variance pattern as other categorical variables

6. **REGION**:
   - Different spreads observed across regions (0-1 range)
   - Geographic heterogeneity in variance

**Conclusion**:\n\n
Clear signs of heteroskedasticity across multiple predictors, suggesting violations of the constant variance assumption.
")
}
```

```{r Heteroskedasticity_test, echo=FALSE, results='asis'}
cat("#### **4d.1 Heteroskedasticity Tests**\n\n")

```

```{r echo=FALSE, results="asis"}
cat("#### **4.1.1 White's general test**\n\n")
```


```{r White_test , echo=TRUE , results="asis"}

white_test <- bptest(linear_model1, ~ fitted(linear_model1) + I(fitted(linear_model1)^2))
print(white_test)
```

```{r White_ interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" The p value of 0.02977 says that we  reject the null hyptothis , which implies that we reject homoskedastiscity\n
**Properties:**\n\n
- For White there are no properties")
}
```

```{r echo=FALSE, results="asis"}
cat("#### **4.1.2 Goldfeld-quant test**\n\n")
```

```{r QF_test , echo=TRUE , results="asis"}
gqtest_result <- gqtest(linear_model1)
print(gqtest_result)
```


```{r GF_ interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" The p value of p-value =
5.067e-09 says that we strongly reject the null hyptothis , which implies that we reject homoskedastiscity.

**Properties:**\n\n
- You split the dataset into two non-overlapping groups (dropping middle observations).\n
- Errors are normally distributed.\n\n
\n\n")}
```

```{r Implications_for OLS , echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" **4d.2 Implications for OLS Estimators**\n\n
- The coef( the betas ) are not normally distr in small samples\n\n
- Var of the coef changes , we have a more robust variance formula if we use OLS \n\n
- OLS is no longer effecient , we use UGLS wich has a lower variance of the estimator\n\n
- The st errors of the coef are underestimated and are not to be trusted, therefore infernce bout the significance is wrong\n\n")
}
```

```{r edzs, echo=FALSE,results='asis'}
cat("\n **4d.3 Adressing heteroskedasticity**\n\n")
```

```{r address-heteroskedasticity-robust, echo=FALSE, results="asis"}
# Calculate robust standard errors (HC1 for Stata-like small sample adjustment)
robust_se <- sqrt(diag(vcovHC(linear_model1, type = "HC1")))

# Create publication-quality table with robust SEs
stargazer(linear_model1,
          type = "html",  
          title = "OLS Regression with Robust Standard Errors",
          se = list(robust_se),
          dep.var.labels = "Weekly Wage",
          covariate.labels = c("Education", "Age", "Race (1=Black)", "SMSA", "Married",
                             "Region 2", "Region 3", "Region 4", "Region 5",
                             "Region 6", "Region 7", "Region 8", "Region 9"),
          notes = "Heteroskedasticity-consistent standard errors in parentheses",
          notes.append = TRUE,
          digits = 4,  # Controls decimal places
          single.row = TRUE,
          model.numbers = FALSE)


```

```{r interpretation hetrosked , echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("**Interpretation:**\n\n
- Robust standard errors are larger than conventional OLS standard errors\n\n
- This pattern confirms the presence of heteroskedasticity in the data\n\n
- The HC1 estimator provides consistent inference under heteroskedasticity\n\n
- Coefficient estimates remain unchanged, but significance levels may differ\n\n")
}

```

```{r echo=FALSE, results="asis"}
cat("**EGLS:** ")
```

```{r EGLS, echo=TRUE,results="asis"}
gls_model <- gls(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  , data = df)
                 weights = varIdent(form = ~1 | group_var)
```

```{r ,echo=FALSE,results='asis'}
stargazer(gls_model,
          type = "html", # Use "html" for HTML output, "latex" for PDF
          title = "EGLS Estimation Results",
          dep.var.labels = "Weekly Wages",
          covariate.labels = c("Education", "Age", "Race (1=Black)", "SMSA", "Married",
                             "Region 2", "Region 3", "Region 4", "Region 5",
                             "Region 6", "Region 7", "Region 8", "Region 9"),
          notes = "Standard errors in parentheses",
          notes.append = TRUE,
          model.numbers = FALSE,
          column.sep.width = "1pt",
          single.row = TRUE)

if (show_interpretation) {
  cat("\n##### **Comparison with Robust Standard Errors:**\n")
  cat("- EGLS provides more efficient estimates than OLS with robust standard errors\n")
  cat("- Standard errors are typically larger than OLS with incorrect variance formula\n")
  cat("- The efficiency gain comes from properly modeling the heteroskedasticity structure\n")
  cat("- Interpretation of coefficients remains the same as OLS\n")
}
```

```{r Autocorr, echo=FALSE, results='asis'}
cat("#### **4e. Autocorrelation**\n\n")
```

```{r autocorrelation graph, echo=FALSE, results='asis', fig.align='center'} 
df$residuals <- residuals
df_ordered <- df[order(df$EDUC, df$AGE), ]
plot(df_ordered$residuals, type = "l",
     main = "Residuals Ordered by EDUC and AGE",
     ylab = "Residuals", xlab = "Observation Index")
```

```{r autocorrelation runs test, echo=FALSE, results='asis'}
runs.test(df$residuals)
if (show_interpretation) {
  cat("**Assumptions:**
  
1) no need to assume specific error structure

2) The observations should be in a meaningful order as the test evaluates the sequence

High p-value suggest that there is no autocorrelation.")
  }
```

```{r autocorrelation DW test, echo=FALSE, results='asis'}
DW=dwtest(linear_model1)
# print to screen
DW_summary=c(DW$statistic,DW$p.value)
names(DW_summary)=c("Test-statistic","P-value")
stargazer(DW_summary,type="text")
if (show_interpretation) {
  cat("**Assumptions:**
  
1) Error terms follow an AR(1) process

2) Error terms are normally distributed

3) The regression model does not include lagged dependent variables

High p-value suggest that there is no autocorrelation.")
  }
```

```{r autocorrelation BG test, echo=FALSE, results='asis'}
BG = bgtest(linear_model1, order = 6)  # Change order as needed
BG_summary=c(BG$statistic,BG$p.value)
names(BG_summary)=c("Test-statistic","P-value")
stargazer(BG_summary,type="text")
if (show_interpretation) {
  cat("**Assumptions:**
  
1) Allows for stochastic regressors

2) Tests for higher-order autocorrelation

3) Allows for lagged dependent variables

High p-value suggest that there is no autocorrelation.")
  }
```

```{r autocorrelation implications for OLS, echo=FALSE, results='asis'}
if (show_interpretation) {
cat("**Implications for OLS if autocorrelation is present:**
  
1) Standard variance formula is incorrect

2) OLS estimator no longer efficient")

  
cat("Inference: Standard errors are incorrect, leading to invalid t-tests, F-tests, and confidence intervals")
cat("It does not make sense to interpret autocorrelation as a structural feature since there is no natural ordering for the dataset.")
  }
```