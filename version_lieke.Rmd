
---
title: "Case Solution Group 5"
author: "Balint Keller, Ling-Fei Chen , Elisabeth Bonte, Lieke Vanhaverbeke, Stef
  Desender, Seppe Van Campe"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
df_print: paged
---

```{r setup, include=FALSE}
# Global setting: Toggle to TRUE for feedback version, FALSE for exam version
show_interpretation <- FALSE

#load libraries
library(stargazer)
library(psych)
library(ggplot2)
library(car)
library(tseries)
library(lmtest)
library(dplyr)
library(purrr)
library(corrplot)
library(randtests)
library(sandwich)
library(nlme)
library(knitr)
library(kableExtra)
library(broom)
library(modelsummary)
# Load dataset
df <- read.csv(file="Data_Sub_AK91.csv",header=T,sep = ",")
df <- df %>% mutate(
REGION1 = as.integer(REGION == 1), 
REGION2 = as.integer(REGION == 2), 
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9),

QOB1 = as.integer(QOB == 1),
QOB2 = as.integer(QOB == 2),
QOB3 = as.integer(QOB == 3),
QOB4 = as.integer(QOB == 4)
)
```

```{r hypothesis_test, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("# **1. Empirical Specification and Hypotheses**\n\n")

cat("Baseline model:\n\n")
cat("$$WAGE_i = \\beta_1 + \\beta_2 \\, EDUCi + \\alpha \\, X_i + \\mu_i$$\n\n")
cat("Where Xi includes AGE, RACE, MARRIED, SMSA, and REGION.\n\n")

cat("## **Hypothesis Tests:**\n\n")

cat("##### **EDUC: Education**\n\n")
cat("H0: B2 $\\leq$ 0 → Education has no or a negative effect on wages.\n\n")
cat("H1: B2 $>$ 0 → Education has a positive effect on wages.\n\n")
cat("One-sided test: Economic theory predicts that more education increases wages, so a negative effect would be counterintuitive.\n\n")

cat("##### **AGE: Age**\n\n")
cat("H0: A(AGE) $\\leq$ 0 → Age has no or a negative effect on wages.\n\n")
cat("H1: A(AGE) $>$ 0 → Age has a positive effect on wages.\n\n")
cat("One-sided test: Age is often associated with experience, which is expected to increase wages.\n\n")

cat(" ##### **MARRIED: Marital Status**\n\n")
cat("H0: A(MARRIED) $\\leq$ 0 → Being married has no or a negative effect on wages.\n\n")
cat("H1: A(MARRIED) $>$ 0 → Being married has a positive effect on wages.\n\n")
cat("One-sided test: Being married may indicate stability and a positive impact on productivity, leading to higher wages.\n\n")

cat(" ##### **SMSA: Metropolitan Area**\n\n")
cat("H0: A(SMSA) $\\leq$ 0 → Living in a metropolitan area has no or a negative effect on wages.\n\n")
cat("H1: A(SMSA) $>$ 0 → Living in a metropolitan area has a positive effect on wages.\n\n")
cat("One-sided test: Urban areas typically have more job opportunities and higher wages, driving this expectation.\n\n")

cat(" ##### **RACE: Race (1 = Black, 0 = White)**\n\n")
cat("H0: A(RACE) $\\geq$ 0 → Race has no or a positive effect on wages.\n\n")
cat("H1: A(RACE) $<$ 0 → Race has a negative effect on wages.\n\n")
cat("One-sided test: Structural inequalities often result in lower wages for minority groups.\n\n")

cat("##### **REGION: Geographic Region**\n\n")
cat("H0: A(REGION) $\\neq$ 0 → (There are no positive wage differences across regions.)\n\n")
cat("H1: A(REGION) $=$ 0 → (At least one region has a positive effect on wages.)\n\n")
cat("Two-sided test: Region could impact wages in either direction\n\n")

}
```

# Step 1: Specification, hypotheses, and descriptive statistics

```{r head, echo=TRUE}
head(df)
```

```{r summary, echo=TRUE}
summary(df)
```


```{r describe, echo=TRUE}
describe(df)
```

```{r cov, echo=FALSE}
numeric_data <- df %>%
  select(where(is.numeric)) %>%
  select(-matches("^REGION[0-9]+"), -starts_with("QOB"))
```

```{r covv, echo=TRUE}
round(cov(numeric_data), 3)

round(cor(numeric_data), 3)
```

```{r statisctics per region, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **2.1 Statistics per region**\n\n")  # Adds a Markdown header for clarity
  cat("WAGE: Largest wage disparities in regions 2, 6, and 9; high SD, skewness, and kurtosis → outliers with very high incomes.\n\n")
  cat("EDUC: Highest in region 9, lowest in region 6; slightly more variation in regions 5 and 6.\n\n")
  cat("AGE: Almost uniformly ~45 years across regions; low spread, minimal differences.\n\n")
  cat("RACE: Mostly white population (>90%); region 5 relatively more diversity; high kurtosis indicates outliers.\n\n")
  cat("SMSA: Regions 4 & 6 are more urban, region 2 is the most rural; skewed → most live outside urban areas.\n\n")
  cat("MARRIED: High marriage rates across all regions (~85–89%); region 9 slightly lower.\n\n")
}
```

```{r statisctics per region code, echo=FALSE}
library(dplyr)
library(psych)


df %>%
  group_by(REGION) %>%
  summarize(
    mean_wage = mean(WAGE, na.rm = TRUE),
    mean_educ = mean(EDUC, na.rm = TRUE),
    mean_age = mean(AGE, na.rm = TRUE),
    
    sd_wage = sd(WAGE, na.rm = TRUE),
    sd_educ = sd(EDUC, na.rm = TRUE),
    sd_age = sd(AGE, na.rm = TRUE),
    
    var_wage = var(WAGE, na.rm = TRUE),
    var_educ = var(EDUC, na.rm = TRUE),
    var_age = var(AGE, na.rm = TRUE),
    
    
    count = n()
  )

#dit is hoe polina het toont in feedback (ook zelfde uitkomsten als haar)
#alleen de relevante kolommen (alle numerieke behalve dummies)
numeric_vars <- df %>% 
  select_if(is.numeric) %>%
  select(-starts_with("REGION"), -starts_with("QOB"))  # dummies eruit

#combineer met regio
```

```{r statisctics per region codee, echo=FALSE}
df_subset1 <- df %>% 
  select(REGION) %>% 
  bind_cols(numeric_vars)
describeBy(df_subset1, group = "REGION")
```


```{r statisctics per age, echo=FALSE, results="asis"}
if (isTRUE(show_interpretation)) {
  cat("### **2.2 Statistics per age**\n\n")
  cat("AGE GROUPS:\n\n")
  cat("AGE >= 40 & AGE < 42 ~ '40-41 GROUP 1',\n\n")
  cat("AGE >= 42 & AGE < 44 ~ '42-43 GROUP 2',\n\n")
  cat("AGE >= 44 & AGE < 46 ~ '44-45 GROUP 3',\n\n")
  cat("AGE >= 46 & AGE < 48 ~ '46-47 GROUP 4',\n\n")
  cat("AGE >= 48 & AGE <= 50 ~ '48-50 GROUP 5'\n\n")
}
```


```{r statisctics per age code, echo=FALSE}
df_subset2 <- df %>%
  mutate(AGE_GROUP = case_when(
    AGE >= 40 & AGE < 42 ~ "40-41",
    AGE >= 42 & AGE < 44 ~ "42-43",
    AGE >= 44 & AGE < 46 ~ "44-45",
    AGE >= 46 & AGE < 48 ~ "46-47",
    AGE >= 48 & AGE <= 50 ~ "48-50"
  )) %>%
  select(-starts_with("REGION"), -starts_with("QOB"))

describeBy(df_subset2, group = "AGE_GROUP")
```


```{r cov per regio, echo=FALSE , results='asis'}
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    knitr::kable(cov(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED), use = "pairwise.complete.obs"), 
             caption = paste("REGION", region_name),
             digits = 3)
  })

```


```{r cor per regio, echo=FALSE , results='asis'}
df %>%
  group_by(REGION) %>%
  group_split() %>%
  map(~ {
    region_name <- as.character(unique(.x$REGION))  # Extracts region name as text
    knitr::kable(cor(select(.x, WAGE, EDUC, AGE, RACE, SMSA, MARRIED),  use = "pairwise.complete.obs"),
             caption = paste("REGION", region_name),
             digits = 3)
  })

```

```{r plot, echo=FALSE}
ggplot(df, aes(x = df$EDUC, y = df$WAGE)) +
  geom_point(color = "blue") +
  labs(title = " ",
       x = "Years of Education",
       y = "Weekly Wage (in $)")+

  theme_minimal()

```

\newpage
# Step 2: OLS estimation and testing

```{r linear model, echo=TRUE}
linear_model1=lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  , data = df)
```

```{r linear modell, echo=FALSE}
if (show_interpretation) {
  cat("WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9")
}
```

```{r stargazer lin model, echo=FALSE}
stargazer(linear_model1,type="text",style="all")
```


```{r betekenins coef, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("### **3.1 Interpretation of the estimated Coefficient **\n\n")
  cat("\n EDUC (26.696, p < 0.001)

A one-year increase in education leads to a $26.696 increase in weelky wages.
This is highly significant (p = 0.000).

AGE ( 2.244, p = 0.020)
A one-year increase in age increases wages by $2.23.
Significant at the 5% level (p = 0.020).

RACE (-77.478, p < 0.001)
Suggests a wage penalty of $77.48 for certain racial groups (assuming a binary variable where non-white = 1).
Highly significant (p = 0.000).

SMSA (-63.654, p < 0.001)
Living in an SMSA (Standard Metropolitan Statistical Area) is associated with a $63.65 lower wage.
Significant at p = 0.000.

MARRIED (77.927, p < 0.001)
Being married increases wages by $77.93.
Highly significant (p = 0.000).

Regional Effects on Wages

Significant Regions:
REGION2 (B = 41.204, p = 0.003)
REGION3 (B = 49.071, p = 0.0003)
REGION9 (B = 63.543, p = 0.00001)
These regions have higher wages compared to the reference region.

Non-Significant Regions:
REGION4, REGION5, REGION6, REGION7, REGION8 (p > 0.05)
These regions do not significantly differ from the reference region in terms of wages.
")
}

```

```{r Joint significance tests, echo=FALSE}
# Joint significance test: Test whether AGE, RACE, MARRIED, and SMSA jointly contribute to explaining WAGE.
linearHypothesis(linear_model1, c("AGE = 0", "RACE = 0", "MARRIED = 0", "SMSA = 0"))

```

```{r interpretatie joint significance tests, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("The joint significance test shows that AGE, RACE, MARRIED, and SMSA jointly contribute significantly to explaining WAGE.\n",
    "The F-statistic is 60.23 with a p-value < 2.2e-16, indicating strong statistical significance.\n",
    "We reject the null hypothesis that these four variables have no joint effect on wages.\n")
}
```

```{r Regional differences, echo=FALSE}
#Assess whether regional differences are statistically significant.
linearHypothesis(linear_model1, c("REGION2 = 0","REGION3 = 0", "REGION4 = 0", "REGION5 = 0", "REGION6 = 0", "REGION7 = 0", "REGION8 = 0", "REGION9 = 0" ))
```

```{r interpretatie van joint significance, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nSince the p-value is extremely small (<0.001), we reject the null hypothesis. This means that at least one of the region coefficients is significantly different from zero, implying that region does have a statistically significant effect on wages.")
}
```

# Step 3: Checking GM assumptions and individual remedies

```{r header 4, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("# **4. Evaluating Gauss-Markov Assumptions and Applying Remedial Measures**\n\n")
  cat("#### **4a. Stochastic Regressors**\n\n")
}
```

```{r GM assumptrions, echo=FALSE, results="asis"}

if (show_interpretation) {
  cat("###**Gauss-Markov assumptions**:
  \n Assumption 1: Linearity in the parameters: CHECK
  \n Assumption 2a: The X -values are fixed over repeated sampling (fixed regressor model) FAIL
  \n Correlation between explanatory variables and error terms:
  ")
  
}
```


```{r stochastic of deterministic, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("All the variables are stochastic.\n\n")
  cat("
Strictly exogenous: AGE, RACE, QOB
- These variables are fixed and not influenced by wages or the error term.

Weakly exogenous: SMSA, REGION, MARRIED
- These variables may be correlated with unobserved factors affecting wages, but are not directly influenced by wages.

Endogenous: EDUC, 
- Potential for reverse causality or correlation with the error term (e.g., higher wages → more education or higher likelihood of marriage).
")
}
```

```{r histogram plot, echo=FALSE}

# Updated upstream


# Residuen opslaan
residuals <- residuals(linear_model1) 
# Histogram van de residuen
ggplot(data.frame(residuals), aes(x = residuals)) +
  geom_histogram(binwidth = 5, color = "black", fill = "blue", alpha = 0.7) +
  labs(title = "Histogram Residuals", x = "Residuals", y = "Frequentie") +
  theme_minimal()
```


```{r assumption 3, echo=FALSE, results="asis"}
print(mean(residuals))
print(t.test(residuals, mu = 0))
if (show_interpretation) {
  cat("Assumption 3: The expected V value of the error terms is zero: CHECK")
}
```


```{r header 4b, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("#### **4b. Normality Error Terms**\n\n")
}
```

```{r QQ plot_layout, echo=FALSE, fig.width=8, fig.height=6}
qqnorm(residuals)
qqline(residuals, col = "red", lwd = 2)
```

```{r jarque bera test_layout, echo=FALSE, results="asis"}
# Jarque-Bera test uitvoeren
jarque.bera.test(residuals)

if (show_interpretation) {
  cat(paste(
    "Since the p value is small, we reject the null hypothesis. Therefore the residuals are NOT normally distributed.",
    "",
    "Implications for OLS:",
    "",
    "1) Unbiasedness:",
    "Non-normal residuals do not affect the unbiasedness of OLS estimators as long as key assumptions (like linearity, exogeneity of regressors, and zero-mean error term) are satisfied.",
    "",
    "2) Efficiency:",
    "OLS estimators may lose their efficiency because normality of residuals is necessary for OLS to be the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov assumptions.",
    "",
    "3) Hypothesis Testing:",
    "The validity of tests (e.g., t-tests and F-tests) and confidence intervals relies on the normality assumption. Non-normal residuals can lead to incorrect p-values.",
    "",
    "4) Heteroscedasticity or Outliers:",
    "Non-normality can signal issues like heteroscedasticity (non-constant error variance), the presence of outliers, or omitted variable bias.",
    "",
    "Remediation:",
    "Based on the histogram and Q-Q Plot which is slightly right-skewed, we can conclude that the residuals deviate slightly from normality, so OLS may still perform adequately, particularly in our large sample where asymptotic normality applies.",
    sep = "\n"
  ))
}
```


```{r multicollin, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("### **4c: Multicollinearity**\n\n")
}

if (show_interpretation) {
  cat("Gauss-Markov assumption (no perfect multicollinearity) is met.

Analysis of Variance Inflation Factors and correlation:
Low VIFs (<5): Most of the variables have VIF values close to 1, including
EDUC (1.07), AGE (1.01), RACE (1.05), SMSA (1.07), and MARRIED (1.02). 
These values suggest minimal multicollinearity, meaning these 
predictors are relatively independent.\n
      
Moderate VIFs (Between 2 and 5): Some regional variables—REGION2 (3.28), 
REGION3 (3.67), REGION4 (2.18), REGION5 (3.51), REGION6 (2.06), 
REGION7 (2.54), REGION8 (1.82), and REGION9 (2.92)—show moderate 
correlation with other predictors. These values indicate some degree 
of collinearity, but they are not alarmingly high.

High VIFs (>5): None of the variables exceed 5, which suggests that severe
multicollinearity is not a major issue in this regression model. But even
if VIF would be high (VIF>5), a large sample (n = 10 000 in this case) and
high variance in the explanatory variables can still lead to precise 
estimates.



Implications for the properties and precision of the OLS estimator:
1) Parameters remain identifiable.

2) Under the CNLRM assumptions, the OLS estimator remains BLUE and normally distributed.

3) OLS estimators exhibit larger variance and covariance, especially the regions.

4) Wider confidence intervals and lower t-statistics 
->variables appear less significantly different from zero, higher
probability of making a Type II error

5) The overall model fit (R2) is largely unaffected: even if individual 
coefficients are insignificant, the F -test may indicate that the coefficients 
are jointly significant, and can be estimated with high precision.

6) Regression coefficients may change substantially\n\n")

}
```

```{r, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("VIF VALUES")
}
vif_values = vif(linear_model1)
library(knitr)
library(kableExtra)

# Stel dat `vif_values` je VIF-vector is
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values)

kable(vif_df, format = "latex", digits = 3, booktabs = TRUE, caption = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"))

```

```{r Heterskedasticity, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("### **4(d) Heteroskedasticity**\n\n")
}
```

```{r residuals_plot, echo=FALSE}
res <- resid(linear_model1)
plot(linear_model1$fitted.values, res,
     main = " Residuals vs. Fitted Values",
     xlab = "Fitted Values",
     ylab = " Residuals",
     pch = 16, col = "black")
abline(h = 0, lty = 2, col = "red")  # Add a reference line at zero

explanatory_vars <- names(linear_model1$model)[-1]  # exclude response variable

# Plot squared residuals vs each predictor
par(mar = c(4, 4, 1, 1))  # Set smaller margins
# Adjust layout if needed
for (var in explanatory_vars) {
  plot(linear_model1$model[[var]], res,
       main = paste(" Residuals vs", var),
       xlab = var,
       ylab = " Residuals",
       pch = 16, col = "black")
  abline(h = 0, lty = 2, col = "red")
}


```

```{r uitleg_plots, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("
### Interpretation of Residual Plots:

1. **Fitted Values**:
   - increase in spread as fitted values increase
   - This pattern indicates potential heteroskedasticity

2. **EDUC**:
   - Initial increase followed by decrease in spread
   - Non-constant variance pattern visible

3. **RACE**:
   - Larger spread observed for white participants
   - Unequal variance between groups
    ( aigan moer white than black , see SMSA)

4. **SMSA**:
   - Non-metropolitan areas show greater spread
   - Variance differs by urban/rural classification, but this is  
   our data is more or less 90 percent non metropolitan (more data , more spread)

5. **MARRIED**:
   - Similar variance pattern as other categorical variables
   - same thing as SMSA , 90 percent of df is married ( so these       are natural spreads , and the problem of heteroskedasticity       will not be here )
6. **REGION**:
   - Different spreads observed across regions (0-1 range)
   - Geographic heterogeneity in variance

**Conclusion**:\n\n
Clear signs of heteroskedasticity across multiple predictors, suggesting violations of the constant variance assumption.
")
}
```

\newpage
```{r Heteroskedasticity_test, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("## **4(d).1**Heteroskedasticity test **\n\n")
}

```

```{r echo=FALSE, results="asis"}

if (show_interpretation) {
  cat("#### **white's general test**\n\n")
}
```


```{r White_test , echo=TRUE , results="markup"}

white_test <- bptest(linear_model1, ~ fitted(linear_model1) + I(fitted(linear_model1)^2))
print(white_test)
```

```{r White_ interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" the p value of 0.02977 says that we  reject the null hyptothis , which implies that we reject homoskedastiscity\n
**properties**\n\n
-for white there are not properties, ( idk zie slide bij verbetring stond in rood , maar ze zei er niks over")
}
```

```{r White_test_trimmed , echo=FALSE , results="markup"}


# Step 1: Define a trimmed version of your data (removing outliers based on WAGE, for example)
# Adjust variable and trimming logic as needed
trimmed_data <- df %>%
  filter(between(WAGE, quantile(WAGE, 0.01, na.rm = TRUE), 
                        quantile(WAGE, 0.99, na.rm = TRUE)))

# Step 2: Refit your linear model on the trimmed dataset
linear_model_trimmed <- lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  ,  data = trimmed_data)

# Step 3: White test on the new model

```

```{r White_testtrimm , echo=TRUE , results="markup"}
white_test_trimmed <- bptest(linear_model_trimmed, 
                             ~ fitted(linear_model_trimmed) + I(fitted(linear_model_trimmed)^2))
print(white_test_trimmed)
```

```{r White_trimmed_interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" even with trimmed date , we have even a smaller p value  reject the null hyptothis , which implies that we reject homoskedastiscity\n
**properties**\n\n
-for white there are not properties")
}
```


```{r echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("#### **goldfeld-quant test**\n\n")
}
```

```{r QF_test , echo=TRUE , results="markup"}
gqtest_result <- gqtest(linear_model1, order.by =df$EDUC ,fraction = 1000)
print(gqtest_result)
```


```{r GF_ interpretie, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" very small p value , we order by educ bcs the plot educ vs residuals was a clear sign of heteroskedasticity, we strongly reject the null hyptothis , which implies that we reject homoskedastiscity

**properties**\n\n
-You split the dataset into two non-overlapping groups( splitting by increasin educ) (dropping middle observations).\n
-Errors are normally distributed., but in our case they are not so be carefull with the interpratation of this test
\n\n
\n\n")
}


```

```{r Implications_for OLS , echo=FALSE, results="asis"}
if (show_interpretation) {
  cat(" **Implications for OLS Estimators**\n\n
-the coef( the betas ) are not normally distr in small samples\n\n
-var of the coef changes , we have a more robust variance formula if we use OLS \n\n
-OLS is no longer effecient , we use EGLS wich has a lower variance of the estimator\n\n
-the st errors of the coef are underestimated and are not to be trusted, therefore infernce bout the significance is wrong \n\n")
}
```

```{r edzs, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("## **4(d).2 adressing heteroskedasticity**\n\n")
}
```


```{r address-heteroskedasticity-robust, echo=FALSE, results="asis"}
robust_se <- sqrt(diag(vcovHC(linear_model1, type = "HC0")))
stargazer(linear_model1,
          type = "latex",  
          title = "",
          se = list(robust_se),
          dep.var.labels = "Wage",
          covariate.labels = c("Education", "Age", "Race", "SMSA", "Married",
                             "Region 2", "Region 3", "Region 4", "Region 5",
                             "Region 6", "Region 7", "Region 8", "Region 9"),
          notes = "",
          notes.append = TRUE,
          digits = 4, 
          single.row = FALSE,
          model.numbers = FALSE)
```

```{r interpretation White SE, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\n\n##### Interpretation:\n")
  cat("- Robust standard errors are larger than conventional OLS standard errors\n")
  cat("- This pattern confirms the presence of heteroskedasticity in the data\n") 
  cat("- The HC0 estimator provides consistent inference under heteroskedasticity\n")
  cat("- Coefficient estimates remain unchanged, but significance levels may differ\n")
}

```

 
```{r echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("**EGLS** ")
}
```

```{r EGLS estimation, echo= FALSE, results="asis"}
# Step 1: Get design matrix and response variable
X <- model.matrix(linear_model1)  # Includes intercept & region dummies
Y <- df$WAGE
```


```{r EGLS estimation code, echo= FALSE, results="asis"}
y_hat <- fitted(linear_model1)
X_star <- X / y_hat
Y_star <- Y / y_hat 

egls_model <- lm(Y_star ~ X_star - 1)
```

```{r EGLS estimation geen code, echo= FALSE, results="asis"}
stargazer(egls_model,
          type = "latex",
          title = "",
          dep.var.labels = "WAGE",
          covariate.labels = c("Intercept", "EDUC", "AGE", "RACE", "SMSA", "MARRIED",
                             "REGION2", "REGION3", "REGION4", "REGION5",
                             "REGION6", "REGION7", "REGION8", "REGION9"),
          notes = "",
          notes.append = TRUE,
          digits = 4) 

# Show table in basic format
modelsummary(
    list("1" = linear_model1, "2" = egls_model),
    coef_rename = c(
        "X_starEDUC" = "EDUC", 
        "X_starAGE" = "AGE", 
        "X_starRACE" = "RACE",
        "X_starMARRIED" = "MARRIED", 
        "X_starSMSA" = "SMSA",
        "X_star(Intercept)" = "(Intercept)",
        "X_starREGION_factor2" = "REGION_factor2", 
        "X_starREGION_factor3" = "REGION_factor3",
        "X_starREGION_factor4" = "REGION_factor4", 
        "X_starREGION_factor5" = "REGION_factor5",
        "X_starREGION_factor6" = "REGION_factor6", 
        "X_starREGION_factor7" = "REGION_factor7",
        "X_starREGION_factor8" = "REGION_factor8", 
        "X_starREGION_factor9" = "REGION_factor9"
    ),
    vcov=list(vcovHC(linear_model1, type="HC0"),NULL),
    stars=TRUE
)

if (show_interpretation) {
  cat("\n##### **Comparison with Robust Standard Errors:**\n")
  cat("- EGLS provides more efficient estimates than OLS with robust standard errors\n")
  cat("- Standard errors are typically larger than OLS with incorrect variance formula\n")
  cat("- The efficiency gain comes from properly modeling the heteroskedasticity structure\n")
  cat("- Interpretation of coefficients remains the same as OLS\n")
}
```

\newpage  
```{r autocor, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("### **4(e) Autocorrelation**\n\n")
}
```

```{r ordering data, echo=FALSE,results='asis'}
df_ordered <- df %>% arrange(EDUC)
df_ordered_age <- df %>% arrange(AGE)

# Re-run baseline linear model on ordered data
linear_model_ordered <- lm(
  WAGE ~ EDUC + AGE + RACE + SMSA + MARRIED + 
  REGION2 + REGION3 + REGION4 + REGION5 + 
  REGION6 + REGION7 + REGION8 + REGION9,
  data = df_ordered
)

linear_model_ordered_age <- lm(
  WAGE ~ EDUC + AGE + RACE + SMSA + MARRIED + 
  REGION2 + REGION3 + REGION4 + REGION5 + 
  REGION6 + REGION7 + REGION8 + REGION9,
  data = df_ordered_age
)
# Store residuals
residuals_ordered <- residuals(linear_model_ordered)
residuals_ordered_age <- residuals(linear_model_ordered_age)

plot(residuals_ordered, 
    type = "p",  # "p" for points instead of "l" for lines
     pch = 16,    # Solid circle points (other options: 1-25)
     col = "blue", # Color of points
     cex = 0.4,   # Size of points
     main = "Residuals Ordered by EDUC",
     ylab = "Residuals", 
     xlab = "Ordered index by EDUC")
abline(h = 0, lty = 2, col = "red")  # Reference line

plot(residuals_ordered_age, 
    type = "p",  # "p" for points instead of "l" for lines
     pch = 16,    # Solid circle points (other options: 1-25)
     col = "blue", # Color of points
     cex = 0.4,   # Size of points
     main = "Residuals Ordered by AGE",
     ylab = "Residuals", 
     xlab = "Ordered index by AGE")
abline(h = 0, lty = 2, col = "red")  # Reference line
```


```{r runstest, echo=FALSE, results="markup"}
runs_test_ordered <- runs.test(residuals_ordered)
print(runs_test_ordered)
```

```{r runs interpretation, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("We have a very low p-value which means we reject the null hypothesis of no autocorrelation ")
}
```

```{r dw, echo=FALSE, results="markup"}
# Run Durbin-Watson test
dw_test_ordered <- dwtest(linear_model_ordered)
print(dw_test_ordered)
```

```{r DW interpretation, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("Assumptions:
  
1) Error terms follow an AR(1) process

2) Error terms are normally distributed

3) The regression model does not include lagged dependent variables 

4) The regressors are non-stochastic

Since assumptions 1, 2 and 4 are violated, this might not be the best test")
}
```

```{r autocorrelation BG test, echo=FALSE, results="markup"}
bg_test_ordered <- bgtest(linear_model_ordered, order = 6)  # Test up to 3 lags
print(bg_test_ordered)
```

```{r BG interpretation, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("Assumptions:
  
  1) Allows for stochastic regressors
  
  2) Tests for higher-order autocorrelation
  
  3) Allows for lagged dependent variables")  
}
```

```{r autocorrelation implications for OLS, echo=FALSE,results='asis'}
if (show_interpretation) {
  cat("Implications for OLS if autocorrelation is present:
  
  1) Standard variance formula is incorrect
  
  2) OLS estimator no longer efficient
  
  Inference: Standard errors are incorrect, leading to invalid t-tests, F-tests, and     confidence intervals.
  
  It does not make sense to interpret autocorrelation as a structural feature since
  there is no natural ordering for the dataset.")
  }
```

\newpage  
```{r echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("## **4(F). MODEL SPECIFICATION**\n\n")
}
```

```{r, echo=FALSE, results='asis'}
if (show_interpretation) {
  cat("\nIn earlier tests we both saw indications of heteroskedasticity and
      autocorrelation, these two can both be an indicator of model misspecification,
      where we have said in previous part that autocorrelation in particular can 
      be a sign of model misspecification. Further investigation is needed.\n")
}
```

```{r Ramsey_1, echo=FALSE, results="markup"}
Yfit=linear_model1$fitted.values
Ramsey_model<-lm(WAGE~ EDUC + AGE + RACE + SMSA + MARRIED + +I(Yfit^2)+I(Yfit^3)+REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9, data=df)
stargazer(Ramsey_model,type="text",style="all")
# Calculate Ramsey RESET test statistic
n <- nrow(df)  # number of observations
k <- length(coef(linear_model1))  # number of parameters in original model
ssr_original <- sum(resid(linear_model1)^2)
ssr_ramsey <- sum(resid(Ramsey_model)^2)
```

```{r Ramseyy, echo=TRUE,results="markup"}
Ramsey_test <- ((ssr_original - ssr_ramsey)/2)/(ssr_ramsey/(n - k - 2))
p_value <- pf(Ramsey_test, 2, n - k - 2, lower.tail = FALSE)
```

```{r Ramseyyy, echo=FALSE,results="markup"}
Ramsey_summary=c(Ramsey_test,pf(Ramsey_test, 2, 60, lower.tail = FALSE))
names(Ramsey_summary)=c("Test-statistic","P-value")
stargazer(Ramsey_summary,type="text")
```

```{r Ramsey, echo=TRUE,results="markup"}
reset_test <- resettest(linear_model1, power = 2:3, type = "fitted")
print(reset_test)
```

```{r LM test educ en agee, echo=TRUE,results="markup"}
res=linear_model1$residuals
LM_reg=lm(res~EDUC+I(EDUC^2)+I(EDUC^3)+ AGE +I(AGE^2)+ I(AGE^3)+ RACE + 
            SMSA + MARRIED + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 
          + REGION7 + REGION8 + REGION9, data = df)
```

```{r LM test educ en age, echo=FALSE,results="markup"}
stargazer(LM_reg,type="text",style="all") #mag verwijderd worden, niet nodig op examen
LM_test = 10000*summary(LM_reg)$r.squared
LM_summary=c(LM_test,pchisq(LM_test,df=2,lower.tail=FALSE))
names(LM_summary)=c("Test-statistic","P-value")
stargazer(LM_summary,type="text")
```

```{r intr, echo=FALSE, results='asis'}
if (show_interpretation){
cat("Strongly reject the null hypothesis that the model is correctly specified. The test shows that nonlinear functions of EDUC and AGE significantly explain variation in the residuals — i.e., the current baseline misses important nonlinear structure.")}
```

```{r LM test educc, echo=TRUE,results="markup"}
res=linear_model1$residuals
LM_reg_educ=lm(res~EDUC+I(EDUC^2)+I(EDUC^3)+ AGE + RACE + SMSA + MARRIED + REGION2
               + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9, data = df)
```

```{r LM test educ, echo=FALSE,results="markup"}
stargazer(LM_reg_educ,type="text",style="all")
LM_test = 10000*summary(LM_reg_educ)$r.squared
LM_summary=c(LM_test,pchisq(LM_test,df=2,lower.tail=FALSE))
names(LM_summary)=c("Test-statistic","P-value")
stargazer(LM_summary,type="text")
```

```{r LM test agee, echo=TRUE,results="markup"}
res=linear_model1$residuals
LM_reg_age=lm(res~EDUC+I(AGE^2)+I(AGE^3)+ AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9, data = df)
```

```{r LM test age, echo=FALSE,results="markup"}
stargazer(LM_reg_age,type="text",style="all")
LM_test = 10000*summary(LM_reg_age)$r.squared
LM_summary=c(LM_test,pchisq(LM_test,df=2,lower.tail=FALSE))
names(LM_summary)=c("Test-statistic","P-value")
stargazer(LM_summary,type="text")
```

```{r intrr, echo=FALSE, results='asis'}
if (show_interpretation){
  cat("The test results suggest that the omitted non-linearities are due to EDUC but
      not due to AGE -> Try adding EDUC² and EDUC³ to the baseline.")
}
```

```{r log, echo=FALSE,results="markup"}
log_model <- lm(log(WAGE)~ EDUC + AGE + RACE + SMSA + MARRIED + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9  , data = df)
#Robust SEs
baseline_se<-sqrt(diag(vcovHC(linear_model1,,type="HC1")))
log_model_se<-sqrt(diag(vcovHC(log_model,,type="HC1")))
#Stargazer output
stargazer(linear_model1,log_model,
          se = list(baseline_se,log_model_se),
          type="text", title="Baseline vs Log-Linear Model with Robust SEs", column.labels = c("Level", "Log-Level"),
          allign = TRUE, no.space = TRUE)
# compare AIC
AIC(linear_model1, log_model)
# compare BIC
BIC(linear_model1, log_model)
cat("Linear model r-squared:")
summary(linear_model1)$r.squared 
cat("Log model r-squared:")
summary(log_model)$r.squared
```

```{r intrrr, echo=FALSE, results='asis'}
if (show_interpretation){
cat("The log model has a lower AIC and BIC and higher R-squared compared to the linear model, so log model has a better fit")}
```

```{r RESET log, echo=TRUE,results="markup"}
reset_test <- resettest(log_model, power = 2:3, type = "fitted")
print(reset_test)
```

```{r intrrrr, echo=FALSE, results='asis'}
if (show_interpretation){
cat("Reset test still shows misspecification but model has improved compared to baseline.")}
```

```{r, echo=TRUE,results="markup"}
educ_reg <- lm(WAGE ~  EDUC + AGE + RACE + MARRIED + SMSA + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9 + I(EDUC^2), data = df)
anova(linear_model1,educ_reg)

```

```{r, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nHere we can test if the incorporation of higher polynomials is interesting by using the F test, in this case we see that adding educ^2 is interesting. We will further investigate if this is also the case for adding even higher orders of educ\n")
    cat("\nWe need to be careful with datamining, where we fit the model too much to the sample instead of to the population. This can later be (partially) checked by using a holdout sample.\n")
}
```

```{r, echo=TRUE,results="markup"}
educpolythree_reg <- lm(WAGE ~  EDUC + AGE + RACE + MARRIED + SMSA + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9 + I(EDUC^2)+I(EDUC^3), data = df)
anova(educ_reg,educpolythree_reg)

```

```{r, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nUp until order 5, adding higher polynomials of educ is interesting, but when adding educ^5 we don't see a significant improvement.\n")
}
```

```{r, echo=TRUE,results="markup"}
age_reg <- lm(WAGE ~  EDUC + AGE + RACE + MARRIED + SMSA + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9 + I(AGE^2), data = df)
anova(linear_model1,age_reg)

```

```{r, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nHere we don't have a significant result at the 5% level, meaning that we don't see a big enough improvement to add a higher order of age to the model\n")
}
```

```{r, echo=TRUE,results="markup"}
ageinteraction_reg <- lm(WAGE ~  EDUC + AGE + RACE + MARRIED + SMSA + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9 + I(AGE*EDUC), data = df)
anova(linear_model1,ageinteraction_reg)

raceinteraction_reg <- lm(WAGE ~  EDUC + AGE + RACE + MARRIED + SMSA + REGION2 + REGION3 
                 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9 + I(RACE*EDUC), data = df)
anova(linear_model1,raceinteraction_reg)
```

```{r, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nWe will not incorporate the interaction term with age, but the interaction term between race and education seems to have a significant positive result on the fit of the model.\n")
  
  
}
```

```{r code niet in doc, echo=FALSE, results="asis"}
hold <- read.csv("Data_HoldOut_AK91.csv")
hold <- hold %>% mutate(
REGION1 = as.integer(REGION == 1), 
REGION2 = as.integer(REGION == 2), 
REGION3 = as.integer(REGION == 3),
REGION4 = as.integer(REGION == 4),
REGION5 = as.integer(REGION == 5),
REGION6 = as.integer(REGION == 6),
REGION7 = as.integer(REGION == 7),
REGION8 = as.integer(REGION == 8),
REGION9 = as.integer(REGION == 9))
```

```{r chi squared_best, echo=TRUE,results="markup"}
best_mod <- lm(log(WAGE)~ EDUC + I(EDUC^2) + I(EDUC^3) + AGE + RACE + 
                 I(EDUC*RACE) +SMSA + MARRIED + REGION2 + REGION3 + 
                 REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9,
               data = df)
```



```{r code niet in docc, echo=FALSE,results="markup"}
beta=best_mod$coefficients
X_hold <- model.matrix(formula(best_mod),data = hold)
y_hold <- log(hold$WAGE)
y_hat_base <- X_hold %*% beta
residuals_holdout <- y_hold - y_hat_base
# Estimate sigma² from full sample using the BEST model
sigma2_hat <- summary(best_mod)$sigma^2

# Compute chi² test statistic
chi2_stat <- sum(residuals_holdout^2) / sigma2_hat
# Degrees of freedom = # of holdout obs
p_value <- pchisq(chi2_stat, df = nrow(hold), lower.tail = FALSE)

# Report
chi2_summary <- c(chi2_stat, p_value)
names(chi2_summary) <- c("Test-statistic Forecast chi sq", "P-value")

# Output nicely
stargazer(chi2_summary, type = "text", summary = FALSE)
```


```{r, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\n The p-value is 1, so we can't reject the null hypothesis, in other words, there's no misspecification.\n")
  }
```

```{r best model reset, echo=TRUE,results="markup"}
reset_test <- resettest(best_mod, power = 2:3, type = "fitted")
print(reset_test)
```
\newpage  
```{r header 4g, echo=FALSE, results="asis"}
if (show_interpretation) {
cat("#### **4g. Endogeneity and Instrumental Variables**\n\n
    
    ")
}
```


```{r endogeneity, echo=FALSE, results="asis"}
if (show_interpretation) {
cat("We assumed in 4a that Education is endogenous.

- If EDUC is endogenous (i.e., correlated with the error term), the OLS estimator is biased and inconsistent. 

- To solve this, we use the instrumental variables (IV) approach, specifically Two-Stage Least Squares (2SLS). 
This method replaces the endogenous regressor with a predicted version from instruments that are: relevant (correlated with EDUC) and exogenous.

- Proposed instrument and justification: we use Quarter of Birth (QOB) as an instrument for EDUC. 
Justification of choice: QOB affects the age at which individuals can legally drop out of school. QOB is strictly exogenous: it is randomly assigned at birth and unlikely to be 
correlated with unobserved factors that affect wages directly.These factors are more or less uniformly distributed among different QOB.

- Assessing the strength of the instrument: we check the F-statistic of the instrument in the first-stage regression.

- Testing for endogeneity: we perform a Hausman test.
    ")
}
```

```{r header 2SLS, echo=FALSE, results="asis"}
if (show_interpretation) {
cat("#### **2SLS estimation**\n\n
    
    ")
}
```


```{r endogeneity iv model, echo=TRUE, results="markup"}
if (!requireNamespace("AER", quietly = TRUE)) install.packages("AER")
library(AER)

iv_model <- ivreg(WAGE ~ EDUC + AGE + RACE + MARRIED + SMSA + factor(REGION) | 
                    QOB + AGE + RACE + MARRIED + SMSA + factor(REGION),
                  data = df)

summary(iv_model, diagnostics = TRUE)
```

```{r endogeneity iv model met factor, echo=TRUE, results="markup"}
if (!requireNamespace("AER", quietly = TRUE)) install.packages("AER")
library(AER)

iv_modelfactor <- ivreg(WAGE ~ EDUC + AGE + RACE + MARRIED + SMSA + factor(REGION) | 
                    factor(QOB) + AGE + RACE + MARRIED + SMSA + factor(REGION),
                  data = df)

summary(iv_modelfactor, diagnostics = TRUE)
```

```{r endogeneity interpretation, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("Interpretation (2SLS estimation)
In the 2SLS estimation, the coefficient on EDUC is positive (23.29) but not
statistically significant (p = 0.577), indicating no strong evidence of a causal 
effect of education on wages in this IV setup. Additionally, the large standard 
error on the EDUC coefficient (41.78) suggests that the instruments used to 
predict education may be weak. Weak instruments can result in imprecise and 
biased estimates, which limits the reliability of the 2SLS results. This 
potential weakness in the first-stage regression implies that the causal 
effect of education on wages cannot be reliably identified in this model.The 
p-value of 0.00618 for the Weak instruments test indicates that the null 
hypothesis (instruments are weak) is rejected at the 1% significance level. 
However, the F-statistic of 4.130 is below the rule-of-thumb threshold of 10,
which is commonly used to assess instrument strength. While the significant 
p-value suggests the instruments are relevant, the low F-statistic implies they 
may still be weak, potentially leading to biased estimates. 

In contrast, being married significantly increases wages (coef = 78.51, 
p < 0.001), and some regional dummies (REGION2, REGION3, REGION9) are also
statistically significant, suggesting notable regional wage differences. 
Other covariates, such as AGE, RACE, and SMSA, are not significant in this
specification.")
}
```

```{r header first-stage regression, echo=FALSE, results="asis"}
if (show_interpretation) {
cat("#### **First-stage regression (Instrument Relevance)**\n\n
    
    ")
}
```


```{r first stage regr, echo=TRUE, results="markup"}
first_stage <- lm(EDUC ~ AGE + RACE + MARRIED + SMSA + factor(REGION)+ QOB, data = df)
summary(first_stage)
```

```{r first stage regr met factor, echo=TRUE, results="markup"}
first_stagefactor <- lm(EDUC ~ AGE + RACE + MARRIED + SMSA + factor(REGION)+ factor(QOB), data = df)
summary(first_stagefactor)
```

```{r first stage interpretation, echo=FALSE, results="asis"}
if (show_interpretation) {
cat("In the first-stage regression, the instrument QOB is statistically 
    significant (p = 0.038), with a positive coefficient (0.0595), 
    indicating that it is correlated with the endogenous regressor EDUC. 
    Additionally, the F-statistic of the joint significance of the 
    instruments is 55.7, which is well above the conventional threshold 
    of 10. This suggests that the instrument is not weak and satisfies 
    the relevance condition for valid instrumental variable estimation.")
}
```

```{r header Hausman, echo=FALSE, results="asis"}
if (show_interpretation) {
cat("#### **Hausman test**\n\n
    
    ")
}
```

```{r hausman zonder factor, echo=TRUE, results="markup"}
first_stage <- lm(EDUC ~ QOB + AGE + RACE + MARRIED + SMSA + factor(REGION), data = df)
df$first_stage_residuals <- residuals(first_stage)

augmented_modelHausman <- lm(WAGE ~ EDUC + AGE + RACE + MARRIED + SMSA + REGION2 
                      + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 
                      + REGION8 + REGION9 + first_stage_residuals, data = df)
summary(augmented_modelHausman)

```

```{r hausman, echo=TRUE, results="markup"}
first_stagefactor <- lm(EDUC ~ factor(QOB) + AGE + RACE + MARRIED + SMSA + factor(REGION), data = df)
df$first_stage_residuals <- residuals(first_stagefactor)

augmented_modelHausmanfactor <- lm(WAGE ~ EDUC + AGE + RACE + MARRIED + SMSA + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9 + first_stage_residuals, data = df)
summary(augmented_modelHausmanfactor)

```


```{r hausman interpretation, echo=FALSE, results="asis"}
if (show_interpretation) {
cat("The coefficient on the first-stage residuals is not statistically 
significant (p = 0.935), suggesting that there is no strong 
evidence of endogeneity in EDUC. This implies that OLS is consistent,
and the use of an instrumental variable approach is not strictly 
necessary in this case.

However, the standard error on the EDUC coefficient remains high 
(41.75), reflecting the imprecision observed in the 2SLS estimates. 
This may be due to a weak instrument or insufficient power in the sample.
Therefore, although the Hausman test does not reject the consistency of OLS,
2SLS may still be theoretically preferred if there are substantive concerns
about omitted variable bias or reverse causality.")
}
```

# Step 4: General remedies and conclusions

```{r, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\n\nRecap of stage 4:\n")
  cat("\n(a) Stochastic regressors\n")
  
  cat("\n\n - We have no determanistic regressors. most regressors are exogenous (weak or strict) and EDUC is endogenous. remediation for exogenous variables are not needed, but they make our model biased (but still consistent). Endogeneity is discussed later.\n")
  cat("\n --> when exogenous: asumptotically efficient, asymptotically normal, consistent. introduction of an apporximation error in small samples\n")
  
  cat("\n\n(b) Error terms normally distr.\n")
  cat("\n - Our errors are not normally distributed. But: We don't need remediation, Since the Central Limit Theorem ensures that the estimators are asymptotically normal in large samples. Inference remains valid.\n")
  cat("\n--> still BLUE\n")

  cat("\n\nMulticollinearity\n")
  cat("\n - Both the pairwise correlations and the Variance inflation factors don't show indications of (extreme amounts of) Multicollinearity, so there is a lot of evidence that there is no problem for our OLS in this part\n")
  
  cat("\n\n(pure) Heteroskedasticity\n")
  cat("\n - We have (pure) Heteroskedasticity, so we need a remediation. This can be done via GLS or robust variance. Dependent on the solution we opt for whe have different consequences.\n")
  cat("\n - (E)GLS: If the weights can be estimated: EGLS is consistent and asymptotically efficient\n")
  cat("\n - Robust variance: If you do this via 'White’s heteroskedasticity-consistent variance estimator', asymptotically valid but is biased in small samples (not the case for us). OLS is also not efficient anymore\n")
  
  cat("\n\n(pure) Autocorrelation\n")
  cat("\n - since we work with cross sectional data, we don't think we have pure Autocorrelation.\n")
  
  cat("\n\nModel specification\n")
  cat("\n - dependent on what type of model specification errors you have you have different consequences. in part 4(f) we made a new model that did good on the chi^2 test.\n")
  cat("\n - The model: lm(WAGE ~  EDUC + AGE + RACE + MARRIED + SMSA + REGION_factor + I(EDUC^2)+I(EDUC^3)+I(EDUC^4) + I(RACE*EDUC), data = df)\n")
  
  cat("\n\nEndogeneity\n")
  cat("\n The Hausman test showed us that there is no Endogeneity\n")
  
  cat("\n\n We would like to combine all these methods into one final model. Here our focus is on  model specification & heteroskedasticity, besides these we also had some outliers which we would like to remove.\n")
}
```





```{r, echo=FALSE, results="asis"}
if (show_interpretation) {
  cat("\nThis gives us a final model, where we both account for heteroskedasticity (via EGLS) and for model misspecification. A disadvantage here is that EGLS is only an estimate, but this problem can not be further dealt with.\n\n")
  cat("\n\nWe based our model specification on previous findings.\n")
}
```

```{r FINAAL MODEL via 2sls, echo=TRUE, results="markup"}

library(AER)
library(lmtest)
library(dplyr)


df <- read.csv(file="Data_Sub_AK91.csv", header=TRUE, sep=",")

df <- df %>%
  mutate(
    REGION1 = as.integer(REGION == 1),
    REGION2 = as.integer(REGION == 2),
    REGION3 = as.integer(REGION == 3),
    REGION4 = as.integer(REGION == 4),
    REGION5 = as.integer(REGION == 5),
    REGION6 = as.integer(REGION == 6),
    REGION7 = as.integer(REGION == 7),
    REGION8 = as.integer(REGION == 8),
    REGION9 = as.integer(REGION == 9)
  )


lower_bound <- quantile(log(df$WAGE), 0.005, na.rm = TRUE)
upper_bound <- quantile(log(df$WAGE), 0.995, na.rm = TRUE)
df_trimmed <- df %>% filter(log(WAGE) >= lower_bound & log(WAGE) <= upper_bound)


df_trimmed <- df_trimmed %>%
  mutate(
    QOB_num = as.numeric(as.character(QOB)),  
    QOB = droplevels(factor(QOB))  
)


stopifnot(nlevels(df_trimmed$QOB) >= 2)


iv_model <- ivreg(
  log(WAGE) ~ EDUC + I(EDUC^2) + I(EDUC^3) + EDUC:RACE + AGE + RACE + MARRIED + SMSA + 
    REGION1 + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9 | 
    QOB_num + I(QOB_num^2) + I(QOB_num^3) + QOB:RACE + AGE + RACE + MARRIED + SMSA + 
    REGION1 + REGION2 + REGION3 + REGION4 + REGION5 + REGION6 + REGION7 + REGION8 + REGION9,
  data = df_trimmed
)


robust_se <- vcovHAC(iv_model)
model_results <- coeftest(iv_model, vcov = robust_se)


print(model_results)



```

